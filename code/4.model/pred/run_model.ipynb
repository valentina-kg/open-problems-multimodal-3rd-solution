{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c93995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade pandas\n",
    "!pip install tables   \n",
    "# necessary for pd.read_hdf()\n",
    "\n",
    "!pip install ipywidgets\n",
    "!pip install --upgrade jupyter\n",
    "!pip install IProgress\n",
    "!pip install catboost\n",
    "!pip install shap\n",
    "!pip install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77b2513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db427e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0dabc",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d337f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrz_path = '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/'\n",
    "\n",
    "# model_path_for_now = '/dss/dsshome1/02/di93zoj/valentina/open-problems-multimodal-3rd-solution/'\n",
    "\n",
    "raw_path =  lrz_path + 'input/raw/'  # '../../../input/raw/'\n",
    "\n",
    "cite_target_path = lrz_path + 'input/target/cite/'   # '../../../input/target/cite/'\n",
    "cite_feature_path = lrz_path + 'input/features/cite/'   # '../../../input/features/cite/'\n",
    "cite_mlp_path = lrz_path + 'model/cite/mlp/'   # '../../../model/cite/mlp/'   # '../../../model/cite/mlp/'\n",
    "cite_cb_path = lrz_path + 'model/cite/cb/'   # '../../../model/cite/cb/'\n",
    "\n",
    "# multi_target_path = lrz_path + 'input/target/multi/'   # '../../../input/target/multi/'\n",
    "# multi_feature_path = lrz_path + 'input/features/multi/'   # '../../../input/features/multi/'\n",
    "# multi_mlp_path = lrz_path + 'model/multi/mlp/'   # '../../../model/multi/mlp/'\n",
    "# multi_cb_path = lrz_path + 'model/multi/cb/'   # '../../../model/multi/cb/'\n",
    "\n",
    "index_path = lrz_path + 'input/preprocess/cite/'\n",
    "\n",
    "output_path = lrz_path + 'output/'   # '../../../output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b1646",
   "metadata": {},
   "source": [
    "## Cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0eeb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model name\n",
    "#mlp_model_path = os.listdir(cite_mlp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51779e0",
   "metadata": {},
   "source": [
    "markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model names and lists/dict/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569c2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_name = [\n",
    "    'corr_add_con_imp',\n",
    "    'corr_last_v3', \n",
    "    'corr_c_add_w2v_v1_mish_flg',\n",
    "    'corr_c_add_w2v_v1_flg',\n",
    "    'corr_c_add_84_v1',\n",
    "    'corr_c_add_120_v1',\n",
    "    'corr_w2v_cell_flg',\n",
    "    'corr_best_cell_120',\n",
    "    'corr_cluster_cell',\n",
    "    'corr_w2v_128',\n",
    "    'corr_imp_w2v_128',\n",
    "    'corr_snorm',\n",
    "    'corr_best_128',\n",
    "    'corr_best_64',\n",
    "    'corr_cluster_128',\n",
    "    'corr_cluster_64',\n",
    "    'corr_svd_128',\n",
    "    'corr_svd_64',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86dfdfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cite_mlp_corr_add_con_imp_flg_donor_val_50',\n",
       " 'cite_mlp_corr_last_v3_flg_donor_val_55',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_mish_flg_donor_val_66',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_flg_donor_val_66',\n",
       " 'cite_mlp_corr_c_add_84_v1_flg_donor_val_47',\n",
       " 'cite_mlp_corr_c_add_120_v1_flg_donor_val_63',\n",
       " 'cite_mlp_corr_w2v_cell_flg_donor_val_51',\n",
       " 'cite_mlp_corr_best_cell_120_flg_donor_val_51',\n",
       " 'cite_mlp_corr_cluster_cell_flg_donor_val_64',\n",
       " 'cite_mlp_corr_w2v_128_flg_donor_val_42',\n",
       " 'cite_mlp_corr_imp_w2v_128_flg_donor_val_38',\n",
       " 'cite_mlp_corr_snorm_flg_donor_val_39',\n",
       " 'cite_mlp_corr_best_128_flg_donor_val_45',\n",
       " 'cite_mlp_corr_best_64_flg_donor_val_50',\n",
       " 'cite_mlp_corr_cluster_128_flg_donor_val_51',\n",
       " 'cite_mlp_corr_cluster_64_flg_donor_val_57',\n",
       " 'cite_mlp_corr_svd_128_flg_donor_val_30',\n",
       " 'cite_mlp_corr_svd_64_flg_donor_val_38']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_list = []\n",
    "\n",
    "for i in mlp_model_name:\n",
    "    for num, j in enumerate(os.listdir(cite_mlp_path)):\n",
    "        if i in j:\n",
    "            model_name_list.append(j)\n",
    "\n",
    "len(model_name_list)\n",
    "model_name_list   # TODO why not listdir(cite_mlp_path) directly? always 18 models... does mlp_model_name list come from somewhere else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ee5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [1, 0.3, 1, 1, 1, 1, 1, 1, 1, 0.8, 0.8, 0.8, 0.8, 0.5, 0.5, 0.5, 1, 1, 2, 2]\n",
    "weight_sum = np.array(weight).sum()    # == 19\n",
    "\n",
    "# model_feat_dict = {model_name_list[0]:['X_test_add_con_imp.pickle', 1],\n",
    "#                    model_name_list[1]:['X_test_last_v3.pickle', 0.3],\n",
    "#                    model_name_list[2]:['X_test_c_add_w2v_v1.pickle', 1],\n",
    "#                    model_name_list[3]:['X_test_c_add_w2v_v1.pickle', 1],\n",
    "#                    model_name_list[4]:['X_test_c_add_84_v1.pickle', 1],\n",
    "#                    model_name_list[5]:['X_test_c_add_v1.pickle', 1],\n",
    "                   \n",
    "#                    model_name_list[6]:['X_test_feature_w2v_cell.pickle', 1],\n",
    "#                    model_name_list[7]:['X_test_best_cell_128_120.pickle', 1],\n",
    "#                    model_name_list[8]:['X_test_cluster_cell_128.pickle', 1],\n",
    "                   \n",
    "#                    model_name_list[9]:['X_test_feature_w2v.pickle', 0.8],\n",
    "#                    model_name_list[10]:['X_test_feature_imp_w2v.pickle',0.8],\n",
    "#                    model_name_list[11]:['X_test_feature_snorm.pickle', 0.8],\n",
    "#                    model_name_list[12]:['X_test_best_128.pickle', 0.8],\n",
    "#                    model_name_list[13]:['X_test_best_64.pickle', 0.5],\n",
    "#                    model_name_list[14]:['X_test_cluster_128.pickle', 0.5],\n",
    "#                    model_name_list[15]:['X_test_cluster_64.pickle', 0.5],\n",
    "#                    model_name_list[16]:['X_test_svd_128.pickle', 1],\n",
    "#                    model_name_list[17]:['X_test_svd_64.pickle', 1],\n",
    "                   \n",
    "#                    'best_128':['X_test_best_128.pickle', 2],\n",
    "#                    'best_64':['X_test_best_64.pickle', 2],\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db21ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of file names to have model_feat_dict in a cleaner way... but is it still readable? TODO\n",
    "file_names = ['X_test_add_con_imp.pickle',\n",
    " 'X_test_last_v3.pickle',\n",
    " 'X_test_c_add_w2v_v1.pickle',\n",
    " 'X_test_c_add_w2v_v1.pickle',\n",
    " 'X_test_c_add_84_v1.pickle',\n",
    " 'X_test_c_add_v1.pickle',\n",
    " 'X_test_feature_w2v_cell.pickle',\n",
    " 'X_test_best_cell_128_120.pickle',\n",
    " 'X_test_cluster_cell_128.pickle',\n",
    " 'X_test_feature_w2v.pickle',\n",
    " 'X_test_feature_imp_w2v.pickle',\n",
    " 'X_test_feature_snorm.pickle',\n",
    " 'X_test_best_128.pickle',\n",
    " 'X_test_best_64.pickle',\n",
    " 'X_test_cluster_128.pickle',\n",
    " 'X_test_cluster_64.pickle',\n",
    " 'X_test_svd_128.pickle',\n",
    " 'X_test_svd_64.pickle',\n",
    " 'X_test_best_128.pickle',\n",
    " 'X_test_best_64.pickle']\n",
    "\n",
    "model_feat_dict = {name: [file_name, weight] for name, weight, file_name in zip(model_name_list+['best_128', 'best_64'], weight, file_names)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ff044",
   "metadata": {},
   "source": [
    "### cite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd0419cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(x):\n",
    "    x = np.array(x)\n",
    "    return (x - x.mean(1).reshape(-1, 1)) / x.std(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f402fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CiteDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self, feature, target):\n",
    "        \n",
    "#         self.feature = feature\n",
    "#         self.target = target\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.feature)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "                \n",
    "#         d = {\n",
    "#             \"X\": self.feature[index],\n",
    "#             \"y\" : self.target[index],\n",
    "#         }\n",
    "#         return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0734ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteDataset_test(Dataset):\n",
    "    \n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        d = {\n",
    "            \"X\": self.feature[index]\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b013e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def partial_correlation_score_torch_faster(y_true, y_pred):\n",
    "#     \"\"\"Compute the correlation between each rows of the y_true and y_pred tensors.\n",
    "#     Compatible with backpropagation.\n",
    "#     \"\"\"\n",
    "#     y_true_centered = y_true - torch.mean(y_true, dim=1)[:,None]\n",
    "#     y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:,None]\n",
    "#     cov_tp = torch.sum(y_true_centered*y_pred_centered, dim=1)/(y_true.shape[1]-1)\n",
    "#     var_t = torch.sum(y_true_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "#     var_p = torch.sum(y_pred_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "#     return cov_tp/torch.sqrt(var_t*var_p)\n",
    "\n",
    "# def correl_loss(pred, tgt):\n",
    "#     \"\"\"Loss for directly optimizing the correlation.\n",
    "#     \"\"\"\n",
    "#     return -torch.mean(partial_correlation_score_torch_faster(tgt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7411746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num):\n",
    "        super(CiteModel, self).__init__()\n",
    "        \n",
    "        self.layer_seq_256 = nn.Sequential(nn.Linear(feature_num, 256),\n",
    "                                           nn.Linear(256, 128),\n",
    "                                       nn.LayerNorm(128),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        self.layer_seq_64 = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Linear(64, 32),\n",
    "                                       nn.LayerNorm(32),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        self.layer_seq_8 = nn.Sequential(nn.Linear(32, 16),\n",
    "                                         nn.Linear(16, 8),\n",
    "                                       nn.LayerNorm(8),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        \n",
    "        self.head = nn.Linear(128 + 32 + 8, 140)\n",
    "                   \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        from_numpy = False\n",
    "        \n",
    "      ##\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(X)\n",
    "            from_numpy = True\n",
    "        X = X.to(device)  # Move the input to the appropriate device if necessary\n",
    "        ##\n",
    "        X_256 = self.layer_seq_256(X)\n",
    "        X_64 = self.layer_seq_64(X_256)\n",
    "        X_8 = self.layer_seq_8(X_64)\n",
    "        \n",
    "        X = torch.cat([X_256, X_64, X_8], axis = 1)\n",
    "        out = self.head(X)\n",
    "        \n",
    "        if from_numpy:\n",
    "            out = out.cpu().detach().numpy()\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad12b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteModel_mish(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num):\n",
    "        super(CiteModel_mish, self).__init__()\n",
    "        \n",
    "        self.layer_seq_256 = nn.Sequential(nn.Linear(feature_num, 256),\n",
    "                                           nn.Linear(256, 128),\n",
    "                                       nn.LayerNorm(128),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        self.layer_seq_64 = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Linear(64, 32),\n",
    "                                       nn.LayerNorm(32),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        self.layer_seq_8 = nn.Sequential(nn.Linear(32, 16),\n",
    "                                         nn.Linear(16, 8),\n",
    "                                       nn.LayerNorm(8),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        \n",
    "        self.head = nn.Linear(128 + 32 + 8, 140)\n",
    "                   \n",
    "    def forward(self, X, y=None):\n",
    "    \n",
    "        X_256 = self.layer_seq_256(X)\n",
    "        X_64 = self.layer_seq_64(X_256)\n",
    "        X_8 = self.layer_seq_8(X_64)\n",
    "        \n",
    "        X = torch.cat([X_256, X_64, X_8], axis = 1)\n",
    "        out = self.head(X)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7364914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop(model, optimizer, loader, epoch):\n",
    "    \n",
    "#     losses, lrs = [], []\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     #loss_fn = nn.MSELoss()\n",
    "    \n",
    "#     with tqdm(total=len(loader),unit=\"batch\") as pbar:\n",
    "#         pbar.set_description(f\"Epoch{epoch}\")\n",
    "        \n",
    "#         for d in loader:\n",
    "#             X = d['X'].to(device)\n",
    "#             y = d['y'].to(device)\n",
    "            \n",
    "#             logits = model(X)\n",
    "#             loss = correl_loss(logits, y)\n",
    "#             #loss = torch.sqrt(loss_fn(logits, y))\n",
    "        \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             pbar.set_postfix({\"loss\":loss.item()})\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80781ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_loop(model, loader, y_val):\n",
    "    \n",
    "#     model.eval()\n",
    "#     partial_correlation_scores = []\n",
    "#     oof_pred = []\n",
    "    \n",
    "#     for d in loader:\n",
    "#         with torch.no_grad():\n",
    "#             val_X = d['X'].to(device).float()\n",
    "#             val_y = d['y'].to(device)\n",
    "#             logits = model(val_X)\n",
    "#             oof_pred.append(logits)\n",
    "    \n",
    "#     #print(torch.cat(oof_pred).shape, torch.cat(oof_pred).detach().cpu().numpy().shape)\n",
    "#     cor = partial_correlation_score_torch_faster(torch.tensor(y_val).to(device), torch.cat(oof_pred))\n",
    "#     cor = cor.mean().item()\n",
    "#     logits = torch.cat(oof_pred).detach().cpu().numpy()\n",
    "    \n",
    "#     return logits, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9d92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    predicts=[]\n",
    "\n",
    "    for d in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            X = d['X'].to(device)\n",
    "            logits = model(X)\n",
    "            predicts.append(logits.detach().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872f454",
   "metadata": {},
   "source": [
    "### ensemble prediction: use all models, sum weighted predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ff65d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_add_con_imp_flg_donor_val_50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b691a0a74274befbbcc651fb0d80cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_last_v3_flg_donor_val_55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8532ae3b09458c8d51cd028bf97f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_c_add_w2v_v1_mish_flg_donor_val_66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd1c4d3346d4ffeab15c5bd8b11009b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_c_add_w2v_v1_flg_donor_val_66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9230730cf4b42b55d4d9a0522a8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_c_add_84_v1_flg_donor_val_47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782a80e1d149461f9d31f75e36f6763f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_c_add_120_v1_flg_donor_val_63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201faee6aa7b473687fe293309f0543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_w2v_cell_flg_donor_val_51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beacce94f84643a2885a201692ce653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_best_cell_120_flg_donor_val_51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0aa2d17f88452386a66a48ae1797d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_cluster_cell_flg_donor_val_64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1639cb047d4d5c9ac51d64df162eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_w2v_128_flg_donor_val_42\n",
      "cite_mlp_corr_w2v_128_flg_donor_val_42\n",
      "pickle data was truncated\n",
      "cite_mlp_corr_imp_w2v_128_flg_donor_val_38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638c1883411e4f56971ffe021b994ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_snorm_flg_donor_val_39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13268a8cc70d413fa81e2e1a9768b83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_best_128_flg_donor_val_45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3711a77c47f4b2d91826fbe8878d54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_best_64_flg_donor_val_50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd80fa47c347a98a8cd7319446811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_cluster_128_flg_donor_val_51\n",
      "cite_mlp_corr_cluster_128_flg_donor_val_51\n",
      "pickle data was truncated\n",
      "cite_mlp_corr_cluster_64_flg_donor_val_57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de18d1b41d64d4cabcc63841cf25f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_svd_128_flg_donor_val_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5e34e11a9b4bcfa0404e5c986340ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_svd_64_flg_donor_val_38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f6db4d185f46aeb9ee28ec6566c728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c960b47508418980ac93b3cde4e91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc0e022f4f543d08f175bbb4296bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = np.zeros([48203, 140])\n",
    "\n",
    "for num, i in enumerate(model_feat_dict.keys()):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    if 'mlp' in i:\n",
    "\n",
    "        try:\n",
    "            test_file = model_feat_dict[i][0]\n",
    "            test_weight = model_feat_dict[i][1]\n",
    "            X_test = pd.read_pickle(cite_feature_path  + test_file)   \n",
    "            # print(cite_feature_path  + test_file)\n",
    "            X_test = np.array(X_test)\n",
    "            feature_dims = X_test.shape[1]\n",
    "\n",
    "            test_ds = CiteDataset_test(X_test)\n",
    "            test_dataloader = DataLoader(test_ds, batch_size=128, pin_memory=True, \n",
    "                                         shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "            if 'mish' in i:\n",
    "                model = CiteModel_mish(feature_dims)\n",
    "            else:\n",
    "                model = CiteModel(feature_dims)\n",
    "\n",
    "            model = model.to(device)\n",
    "            model.load_state_dict(torch.load(f'{cite_mlp_path}/{i}'))\n",
    "\n",
    "            result = test_loop(model, test_dataloader).astype(np.float32)\n",
    "            result = std(result) * test_weight / weight_sum\n",
    "            pred += result\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e: \n",
    "            print(i)\n",
    "            print(e)             # TODOOOOOOOOOOOOOO\n",
    "        \n",
    "    else:\n",
    "        test_file = model_feat_dict[i][0]\n",
    "        test_weight = model_feat_dict[i][1]\n",
    "        X_test = pd.read_pickle(cite_feature_path  + test_file)\n",
    "        \n",
    "        cb_pred = np.zeros([48203, 140])\n",
    "        \n",
    "        for t in tqdm(range(140)): \n",
    "            cb_model_path = [j for j in os.listdir(cite_cb_path) if f'cb_{t}_{i}' in j][0]\n",
    "            cb = pickle.load(open(cite_cb_path + cb_model_path, 'rb'))\n",
    "            cb_pred[:,t] = cb.predict(X_test)\n",
    "            \n",
    "        cb_pred = cb_pred.astype(np.float32)\n",
    "        pred += std(cb_pred) * test_weight / weight_sum\n",
    "        \n",
    "        del cb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e260220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.325767</td>\n",
       "      <td>-0.487738</td>\n",
       "      <td>-0.351139</td>\n",
       "      <td>1.035394</td>\n",
       "      <td>0.867302</td>\n",
       "      <td>2.263471</td>\n",
       "      <td>3.314403</td>\n",
       "      <td>-0.490992</td>\n",
       "      <td>-0.422089</td>\n",
       "      <td>-0.312560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514007</td>\n",
       "      <td>1.841981</td>\n",
       "      <td>-0.389785</td>\n",
       "      <td>-0.518213</td>\n",
       "      <td>-0.500604</td>\n",
       "      <td>-0.341130</td>\n",
       "      <td>-0.016153</td>\n",
       "      <td>-0.317298</td>\n",
       "      <td>0.038913</td>\n",
       "      <td>0.358591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.293307</td>\n",
       "      <td>-0.470185</td>\n",
       "      <td>-0.351417</td>\n",
       "      <td>1.042340</td>\n",
       "      <td>0.918223</td>\n",
       "      <td>2.295353</td>\n",
       "      <td>3.557569</td>\n",
       "      <td>-0.485068</td>\n",
       "      <td>-0.415240</td>\n",
       "      <td>-0.313825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503055</td>\n",
       "      <td>1.824850</td>\n",
       "      <td>-0.384038</td>\n",
       "      <td>-0.505780</td>\n",
       "      <td>-0.490800</td>\n",
       "      <td>-0.358872</td>\n",
       "      <td>-0.072174</td>\n",
       "      <td>-0.322904</td>\n",
       "      <td>0.059740</td>\n",
       "      <td>0.321151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.337575</td>\n",
       "      <td>-0.449081</td>\n",
       "      <td>-0.328770</td>\n",
       "      <td>1.541549</td>\n",
       "      <td>1.271137</td>\n",
       "      <td>1.486816</td>\n",
       "      <td>4.438023</td>\n",
       "      <td>-0.463581</td>\n",
       "      <td>-0.391279</td>\n",
       "      <td>-0.423727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481228</td>\n",
       "      <td>3.009236</td>\n",
       "      <td>-0.364641</td>\n",
       "      <td>-0.482254</td>\n",
       "      <td>-0.440961</td>\n",
       "      <td>-0.381973</td>\n",
       "      <td>0.338987</td>\n",
       "      <td>-0.273810</td>\n",
       "      <td>0.168474</td>\n",
       "      <td>0.670328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.545628</td>\n",
       "      <td>-0.456024</td>\n",
       "      <td>-0.227837</td>\n",
       "      <td>1.033564</td>\n",
       "      <td>1.055734</td>\n",
       "      <td>0.821383</td>\n",
       "      <td>-0.552602</td>\n",
       "      <td>-0.326394</td>\n",
       "      <td>-0.313855</td>\n",
       "      <td>-0.501781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521892</td>\n",
       "      <td>1.962048</td>\n",
       "      <td>-0.208805</td>\n",
       "      <td>-0.564016</td>\n",
       "      <td>-0.543048</td>\n",
       "      <td>-0.270563</td>\n",
       "      <td>1.487399</td>\n",
       "      <td>-0.492805</td>\n",
       "      <td>1.326041</td>\n",
       "      <td>0.386401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.431234</td>\n",
       "      <td>-0.457296</td>\n",
       "      <td>-0.305963</td>\n",
       "      <td>1.119193</td>\n",
       "      <td>1.234073</td>\n",
       "      <td>1.999678</td>\n",
       "      <td>0.747365</td>\n",
       "      <td>-0.422706</td>\n",
       "      <td>-0.372396</td>\n",
       "      <td>-0.273251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458921</td>\n",
       "      <td>1.382753</td>\n",
       "      <td>-0.385671</td>\n",
       "      <td>-0.452952</td>\n",
       "      <td>-0.536544</td>\n",
       "      <td>-0.331111</td>\n",
       "      <td>0.439839</td>\n",
       "      <td>-0.444841</td>\n",
       "      <td>0.263023</td>\n",
       "      <td>0.104704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48198</th>\n",
       "      <td>-0.054573</td>\n",
       "      <td>-0.461926</td>\n",
       "      <td>-0.341083</td>\n",
       "      <td>1.134846</td>\n",
       "      <td>1.049176</td>\n",
       "      <td>2.120080</td>\n",
       "      <td>3.180188</td>\n",
       "      <td>-0.506037</td>\n",
       "      <td>-0.407075</td>\n",
       "      <td>-0.361818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475566</td>\n",
       "      <td>1.855677</td>\n",
       "      <td>-0.413108</td>\n",
       "      <td>-0.472541</td>\n",
       "      <td>-0.526936</td>\n",
       "      <td>-0.397939</td>\n",
       "      <td>0.197015</td>\n",
       "      <td>-0.195081</td>\n",
       "      <td>-0.057340</td>\n",
       "      <td>0.519309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48199</th>\n",
       "      <td>-0.185888</td>\n",
       "      <td>-0.444854</td>\n",
       "      <td>-0.343561</td>\n",
       "      <td>0.672553</td>\n",
       "      <td>0.861765</td>\n",
       "      <td>2.108776</td>\n",
       "      <td>1.989787</td>\n",
       "      <td>-0.510823</td>\n",
       "      <td>-0.425530</td>\n",
       "      <td>-0.225415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495011</td>\n",
       "      <td>1.518082</td>\n",
       "      <td>-0.410782</td>\n",
       "      <td>-0.496691</td>\n",
       "      <td>-0.531525</td>\n",
       "      <td>-0.411682</td>\n",
       "      <td>0.389263</td>\n",
       "      <td>-0.189678</td>\n",
       "      <td>-0.029223</td>\n",
       "      <td>0.496439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48200</th>\n",
       "      <td>-0.403580</td>\n",
       "      <td>-0.082392</td>\n",
       "      <td>-0.178023</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.854246</td>\n",
       "      <td>1.258473</td>\n",
       "      <td>-0.298337</td>\n",
       "      <td>0.233589</td>\n",
       "      <td>-0.291241</td>\n",
       "      <td>-0.310360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.385333</td>\n",
       "      <td>0.250196</td>\n",
       "      <td>-0.328822</td>\n",
       "      <td>-0.377722</td>\n",
       "      <td>-0.359261</td>\n",
       "      <td>-0.166892</td>\n",
       "      <td>0.372587</td>\n",
       "      <td>-0.334004</td>\n",
       "      <td>-0.239790</td>\n",
       "      <td>0.332295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48201</th>\n",
       "      <td>-0.611163</td>\n",
       "      <td>-0.492633</td>\n",
       "      <td>-0.254988</td>\n",
       "      <td>0.836261</td>\n",
       "      <td>0.370830</td>\n",
       "      <td>1.080112</td>\n",
       "      <td>-0.544935</td>\n",
       "      <td>-0.391174</td>\n",
       "      <td>-0.249658</td>\n",
       "      <td>-0.564887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555495</td>\n",
       "      <td>2.161799</td>\n",
       "      <td>-0.080858</td>\n",
       "      <td>-0.618400</td>\n",
       "      <td>-0.603818</td>\n",
       "      <td>-0.341915</td>\n",
       "      <td>2.621851</td>\n",
       "      <td>-0.519184</td>\n",
       "      <td>2.048543</td>\n",
       "      <td>0.840033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48202</th>\n",
       "      <td>-0.582459</td>\n",
       "      <td>-0.439802</td>\n",
       "      <td>-0.232348</td>\n",
       "      <td>0.938192</td>\n",
       "      <td>0.096082</td>\n",
       "      <td>1.465832</td>\n",
       "      <td>-0.511263</td>\n",
       "      <td>-0.442574</td>\n",
       "      <td>-0.202834</td>\n",
       "      <td>-0.563408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.553082</td>\n",
       "      <td>1.962584</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>-0.639725</td>\n",
       "      <td>-0.592058</td>\n",
       "      <td>-0.308628</td>\n",
       "      <td>3.124003</td>\n",
       "      <td>-0.516436</td>\n",
       "      <td>2.170141</td>\n",
       "      <td>0.980844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48203 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.325767 -0.487738 -0.351139  1.035394  0.867302  2.263471  3.314403   \n",
       "1     -0.293307 -0.470185 -0.351417  1.042340  0.918223  2.295353  3.557569   \n",
       "2     -0.337575 -0.449081 -0.328770  1.541549  1.271137  1.486816  4.438023   \n",
       "3     -0.545628 -0.456024 -0.227837  1.033564  1.055734  0.821383 -0.552602   \n",
       "4     -0.431234 -0.457296 -0.305963  1.119193  1.234073  1.999678  0.747365   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.054573 -0.461926 -0.341083  1.134846  1.049176  2.120080  3.180188   \n",
       "48199 -0.185888 -0.444854 -0.343561  0.672553  0.861765  2.108776  1.989787   \n",
       "48200 -0.403580 -0.082392 -0.178023  0.457895  0.854246  1.258473 -0.298337   \n",
       "48201 -0.611163 -0.492633 -0.254988  0.836261  0.370830  1.080112 -0.544935   \n",
       "48202 -0.582459 -0.439802 -0.232348  0.938192  0.096082  1.465832 -0.511263   \n",
       "\n",
       "            7         8         9    ...       130       131       132  \\\n",
       "0     -0.490992 -0.422089 -0.312560  ... -0.514007  1.841981 -0.389785   \n",
       "1     -0.485068 -0.415240 -0.313825  ... -0.503055  1.824850 -0.384038   \n",
       "2     -0.463581 -0.391279 -0.423727  ... -0.481228  3.009236 -0.364641   \n",
       "3     -0.326394 -0.313855 -0.501781  ... -0.521892  1.962048 -0.208805   \n",
       "4     -0.422706 -0.372396 -0.273251  ... -0.458921  1.382753 -0.385671   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "48198 -0.506037 -0.407075 -0.361818  ... -0.475566  1.855677 -0.413108   \n",
       "48199 -0.510823 -0.425530 -0.225415  ... -0.495011  1.518082 -0.410782   \n",
       "48200  0.233589 -0.291241 -0.310360  ... -0.385333  0.250196 -0.328822   \n",
       "48201 -0.391174 -0.249658 -0.564887  ... -0.555495  2.161799 -0.080858   \n",
       "48202 -0.442574 -0.202834 -0.563408  ... -0.553082  1.962584  0.014673   \n",
       "\n",
       "            133       134       135       136       137       138       139  \n",
       "0     -0.518213 -0.500604 -0.341130 -0.016153 -0.317298  0.038913  0.358591  \n",
       "1     -0.505780 -0.490800 -0.358872 -0.072174 -0.322904  0.059740  0.321151  \n",
       "2     -0.482254 -0.440961 -0.381973  0.338987 -0.273810  0.168474  0.670328  \n",
       "3     -0.564016 -0.543048 -0.270563  1.487399 -0.492805  1.326041  0.386401  \n",
       "4     -0.452952 -0.536544 -0.331111  0.439839 -0.444841  0.263023  0.104704  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "48198 -0.472541 -0.526936 -0.397939  0.197015 -0.195081 -0.057340  0.519309  \n",
       "48199 -0.496691 -0.531525 -0.411682  0.389263 -0.189678 -0.029223  0.496439  \n",
       "48200 -0.377722 -0.359261 -0.166892  0.372587 -0.334004 -0.239790  0.332295  \n",
       "48201 -0.618400 -0.603818 -0.341915  2.621851 -0.519184  2.048543  0.840033  \n",
       "48202 -0.639725 -0.592058 -0.308628  3.124003 -0.516436  2.170141  0.980844  \n",
       "\n",
       "[48203 rows x 140 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c403b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check pred / target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbeb96",
   "metadata": {},
   "source": [
    "### pred loop function (only for single models, not ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e0b2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model_name, X_test=None):     # , rows, cols): \n",
    "\n",
    "    '''if test file from model_feat_dict: pass X_test=None\n",
    "    if other test file, e.g. private test data, pass it as param '''\n",
    "\n",
    "\n",
    "    # pred = np.zeros([rows, cols])\n",
    "    if X_test is None:\n",
    "        test_file = model_feat_dict[model_name][0]\n",
    "        X_test = np.array(pd.read_pickle(cite_feature_path + test_file))\n",
    "    \n",
    "    feature_dims = X_test.shape[1]\n",
    "    print(feature_dims)\n",
    "\n",
    "    test_ds = CiteDataset_test(np.array(X_test))\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=128, pin_memory=True, \n",
    "                                shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    if 'mish' in model_name:\n",
    "        model = CiteModel_mish(feature_dims)\n",
    "    else:\n",
    "        model = CiteModel(feature_dims)\n",
    "        \n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(f'{cite_mlp_path}/{model_name}'))\n",
    "\n",
    "    pred = test_loop(model, test_dataloader).astype(np.float32)\n",
    "   \n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "    return pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3834d1",
   "metadata": {},
   "source": [
    "use for model 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfebb664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef55332d0c04ff59e91fad7dbd90887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.066433</td>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.354774</td>\n",
       "      <td>1.106190</td>\n",
       "      <td>0.869803</td>\n",
       "      <td>3.481040</td>\n",
       "      <td>5.067837</td>\n",
       "      <td>-1.570396</td>\n",
       "      <td>-1.444474</td>\n",
       "      <td>-1.320852</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.585749</td>\n",
       "      <td>2.483784</td>\n",
       "      <td>-1.404202</td>\n",
       "      <td>-1.656756</td>\n",
       "      <td>-1.628249</td>\n",
       "      <td>-1.330432</td>\n",
       "      <td>-1.008113</td>\n",
       "      <td>-1.255932</td>\n",
       "      <td>-0.622936</td>\n",
       "      <td>-0.130905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.067110</td>\n",
       "      <td>-1.506955</td>\n",
       "      <td>-1.399289</td>\n",
       "      <td>1.075526</td>\n",
       "      <td>0.997081</td>\n",
       "      <td>3.506622</td>\n",
       "      <td>5.481764</td>\n",
       "      <td>-1.564466</td>\n",
       "      <td>-1.460833</td>\n",
       "      <td>-1.301538</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.620505</td>\n",
       "      <td>2.777343</td>\n",
       "      <td>-1.404117</td>\n",
       "      <td>-1.657573</td>\n",
       "      <td>-1.687846</td>\n",
       "      <td>-1.422966</td>\n",
       "      <td>-0.780818</td>\n",
       "      <td>-1.299758</td>\n",
       "      <td>-0.628505</td>\n",
       "      <td>-0.121618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.010656</td>\n",
       "      <td>-1.271471</td>\n",
       "      <td>-1.207577</td>\n",
       "      <td>2.074188</td>\n",
       "      <td>1.614956</td>\n",
       "      <td>2.007590</td>\n",
       "      <td>6.589465</td>\n",
       "      <td>-1.374983</td>\n",
       "      <td>-1.260541</td>\n",
       "      <td>-1.450437</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.411724</td>\n",
       "      <td>4.658179</td>\n",
       "      <td>-1.187620</td>\n",
       "      <td>-1.412397</td>\n",
       "      <td>-1.381912</td>\n",
       "      <td>-1.340690</td>\n",
       "      <td>-0.045909</td>\n",
       "      <td>-0.986209</td>\n",
       "      <td>-0.127050</td>\n",
       "      <td>0.551833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506548</td>\n",
       "      <td>-1.336604</td>\n",
       "      <td>-1.056332</td>\n",
       "      <td>0.852848</td>\n",
       "      <td>0.946436</td>\n",
       "      <td>0.652196</td>\n",
       "      <td>-1.553841</td>\n",
       "      <td>-1.178436</td>\n",
       "      <td>-1.245057</td>\n",
       "      <td>-1.485463</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.511633</td>\n",
       "      <td>2.322751</td>\n",
       "      <td>-1.055404</td>\n",
       "      <td>-1.551430</td>\n",
       "      <td>-1.561118</td>\n",
       "      <td>-1.148535</td>\n",
       "      <td>1.396236</td>\n",
       "      <td>-1.462253</td>\n",
       "      <td>1.298150</td>\n",
       "      <td>-0.195608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.272211</td>\n",
       "      <td>-1.349656</td>\n",
       "      <td>-1.142900</td>\n",
       "      <td>1.304754</td>\n",
       "      <td>1.531233</td>\n",
       "      <td>2.833454</td>\n",
       "      <td>0.443823</td>\n",
       "      <td>-1.292403</td>\n",
       "      <td>-1.201001</td>\n",
       "      <td>-1.091781</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.345016</td>\n",
       "      <td>1.609707</td>\n",
       "      <td>-1.278629</td>\n",
       "      <td>-1.350011</td>\n",
       "      <td>-1.648516</td>\n",
       "      <td>-1.214390</td>\n",
       "      <td>0.288555</td>\n",
       "      <td>-1.344913</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>-0.498315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48198</th>\n",
       "      <td>-0.520571</td>\n",
       "      <td>-1.446298</td>\n",
       "      <td>-1.333212</td>\n",
       "      <td>1.226779</td>\n",
       "      <td>1.118786</td>\n",
       "      <td>2.732154</td>\n",
       "      <td>4.920837</td>\n",
       "      <td>-1.530176</td>\n",
       "      <td>-1.416398</td>\n",
       "      <td>-1.343555</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.507962</td>\n",
       "      <td>2.403565</td>\n",
       "      <td>-1.350148</td>\n",
       "      <td>-1.524354</td>\n",
       "      <td>-1.654466</td>\n",
       "      <td>-1.411261</td>\n",
       "      <td>-0.456083</td>\n",
       "      <td>-1.010118</td>\n",
       "      <td>-0.709112</td>\n",
       "      <td>0.126238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48199</th>\n",
       "      <td>-0.641409</td>\n",
       "      <td>-1.391113</td>\n",
       "      <td>-1.307034</td>\n",
       "      <td>0.296210</td>\n",
       "      <td>0.626469</td>\n",
       "      <td>2.656102</td>\n",
       "      <td>2.535754</td>\n",
       "      <td>-1.514254</td>\n",
       "      <td>-1.408760</td>\n",
       "      <td>-1.089353</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.531409</td>\n",
       "      <td>1.995489</td>\n",
       "      <td>-1.341270</td>\n",
       "      <td>-1.576226</td>\n",
       "      <td>-1.658416</td>\n",
       "      <td>-1.425811</td>\n",
       "      <td>-0.163358</td>\n",
       "      <td>-0.952007</td>\n",
       "      <td>-0.691250</td>\n",
       "      <td>0.277180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48200</th>\n",
       "      <td>-1.383966</td>\n",
       "      <td>-0.817277</td>\n",
       "      <td>-1.052890</td>\n",
       "      <td>0.099039</td>\n",
       "      <td>0.724624</td>\n",
       "      <td>1.477175</td>\n",
       "      <td>-1.276211</td>\n",
       "      <td>-0.455357</td>\n",
       "      <td>-1.187938</td>\n",
       "      <td>-1.269387</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.390297</td>\n",
       "      <td>-0.619302</td>\n",
       "      <td>-1.336130</td>\n",
       "      <td>-1.327181</td>\n",
       "      <td>-1.333064</td>\n",
       "      <td>-1.075237</td>\n",
       "      <td>-0.313562</td>\n",
       "      <td>-1.231197</td>\n",
       "      <td>-1.183957</td>\n",
       "      <td>-0.367714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48201</th>\n",
       "      <td>-1.510795</td>\n",
       "      <td>-1.396045</td>\n",
       "      <td>-0.933571</td>\n",
       "      <td>0.539321</td>\n",
       "      <td>-0.054786</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>-1.380013</td>\n",
       "      <td>-1.157287</td>\n",
       "      <td>-1.002030</td>\n",
       "      <td>-1.549959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.488258</td>\n",
       "      <td>2.525875</td>\n",
       "      <td>-0.688867</td>\n",
       "      <td>-1.577495</td>\n",
       "      <td>-1.551894</td>\n",
       "      <td>-1.134680</td>\n",
       "      <td>3.389776</td>\n",
       "      <td>-1.486254</td>\n",
       "      <td>2.734019</td>\n",
       "      <td>0.597246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48202</th>\n",
       "      <td>-1.257885</td>\n",
       "      <td>-1.123296</td>\n",
       "      <td>-0.799638</td>\n",
       "      <td>0.694056</td>\n",
       "      <td>-0.483587</td>\n",
       "      <td>1.462265</td>\n",
       "      <td>-1.332646</td>\n",
       "      <td>-1.137664</td>\n",
       "      <td>-0.821833</td>\n",
       "      <td>-1.349147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.308843</td>\n",
       "      <td>2.070486</td>\n",
       "      <td>-0.469592</td>\n",
       "      <td>-1.463320</td>\n",
       "      <td>-1.352713</td>\n",
       "      <td>-1.021235</td>\n",
       "      <td>3.636977</td>\n",
       "      <td>-1.293923</td>\n",
       "      <td>2.852843</td>\n",
       "      <td>0.811702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48203 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -1.066433 -1.500550 -1.354774  1.106190  0.869803  3.481040  5.067837   \n",
       "1     -1.067110 -1.506955 -1.399289  1.075526  0.997081  3.506622  5.481764   \n",
       "2     -1.010656 -1.271471 -1.207577  2.074188  1.614956  2.007590  6.589465   \n",
       "3     -1.506548 -1.336604 -1.056332  0.852848  0.946436  0.652196 -1.553841   \n",
       "4     -1.272211 -1.349656 -1.142900  1.304754  1.531233  2.833454  0.443823   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.520571 -1.446298 -1.333212  1.226779  1.118786  2.732154  4.920837   \n",
       "48199 -0.641409 -1.391113 -1.307034  0.296210  0.626469  2.656102  2.535754   \n",
       "48200 -1.383966 -0.817277 -1.052890  0.099039  0.724624  1.477175 -1.276211   \n",
       "48201 -1.510795 -1.396045 -0.933571  0.539321 -0.054786  1.035743 -1.380013   \n",
       "48202 -1.257885 -1.123296 -0.799638  0.694056 -0.483587  1.462265 -1.332646   \n",
       "\n",
       "            7         8         9    ...       130       131       132  \\\n",
       "0     -1.570396 -1.444474 -1.320852  ... -1.585749  2.483784 -1.404202   \n",
       "1     -1.564466 -1.460833 -1.301538  ... -1.620505  2.777343 -1.404117   \n",
       "2     -1.374983 -1.260541 -1.450437  ... -1.411724  4.658179 -1.187620   \n",
       "3     -1.178436 -1.245057 -1.485463  ... -1.511633  2.322751 -1.055404   \n",
       "4     -1.292403 -1.201001 -1.091781  ... -1.345016  1.609707 -1.278629   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "48198 -1.530176 -1.416398 -1.343555  ... -1.507962  2.403565 -1.350148   \n",
       "48199 -1.514254 -1.408760 -1.089353  ... -1.531409  1.995489 -1.341270   \n",
       "48200 -0.455357 -1.187938 -1.269387  ... -1.390297 -0.619302 -1.336130   \n",
       "48201 -1.157287 -1.002030 -1.549959  ... -1.488258  2.525875 -0.688867   \n",
       "48202 -1.137664 -0.821833 -1.349147  ... -1.308843  2.070486 -0.469592   \n",
       "\n",
       "            133       134       135       136       137       138       139  \n",
       "0     -1.656756 -1.628249 -1.330432 -1.008113 -1.255932 -0.622936 -0.130905  \n",
       "1     -1.657573 -1.687846 -1.422966 -0.780818 -1.299758 -0.628505 -0.121618  \n",
       "2     -1.412397 -1.381912 -1.340690 -0.045909 -0.986209 -0.127050  0.551833  \n",
       "3     -1.551430 -1.561118 -1.148535  1.396236 -1.462253  1.298150 -0.195608  \n",
       "4     -1.350011 -1.648516 -1.214390  0.288555 -1.344913 -0.039439 -0.498315  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "48198 -1.524354 -1.654466 -1.411261 -0.456083 -1.010118 -0.709112  0.126238  \n",
       "48199 -1.576226 -1.658416 -1.425811 -0.163358 -0.952007 -0.691250  0.277180  \n",
       "48200 -1.327181 -1.333064 -1.075237 -0.313562 -1.231197 -1.183957 -0.367714  \n",
       "48201 -1.577495 -1.551894 -1.134680  3.389776 -1.486254  2.734019  0.597246  \n",
       "48202 -1.463320 -1.352713 -1.021235  3.636977 -1.293923  2.852843  0.811702  \n",
       "\n",
       "[48203 rows x 140 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model #16: cite_mlp_corr_svd_128_flg_donor_val_30\n",
    "get_pred('cite_mlp_corr_svd_128_flg_donor_val_30')\n",
    "\n",
    "# double check train_cite_targets.h5  -> omnipath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796497f",
   "metadata": {},
   "source": [
    "### prediction with private test input -> should get private test target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d6676f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 26867 Ã— 22085\n",
       "    obs: 'kaggle_dataset', 'day', 'donor', 'cell_type'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test_input = ad.read('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/large_preprocessed_files/private_test_input.h5ad')\n",
    "private_test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a91b7954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 26867 Ã— 140\n",
       "    obs: 'kaggle_dataset', 'day', 'donor', 'cell_type'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test_target = ad.read('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/large_preprocessed_files/private_test_target.h5ad')\n",
    "private_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19987e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# private_test_input.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0490b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be non-raw df (non-int)\n",
    "# private_test_target_raw = ad.read('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/large_preprocessed_files/private_test_target_raw.h5ad')\n",
    "# private_test_target_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df423fa0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26867, 212)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('private_X_test_svd.pkl', 'rb') as f:  # private_X_test_svd\n",
    "\n",
    "    private_X_test_svd = pickle.load(f)\n",
    "private_X_test_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9596acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('private_X_test_svd_from_raw.pkl', 'rb') as f:  # private_X_test_svd\n",
    "\n",
    "#     private_X_test_svd_from_raw = pickle.load(f)\n",
    "# private_X_test_svd_from_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c74a0ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ec084c83f4429093f1b113e2b41497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26867, 140)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.373409</td>\n",
       "      <td>-1.285452</td>\n",
       "      <td>-1.181257</td>\n",
       "      <td>0.866390</td>\n",
       "      <td>0.147068</td>\n",
       "      <td>1.610274</td>\n",
       "      <td>5.513695</td>\n",
       "      <td>-1.326972</td>\n",
       "      <td>-1.120104</td>\n",
       "      <td>-0.849544</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.457051</td>\n",
       "      <td>2.210021</td>\n",
       "      <td>-1.219851</td>\n",
       "      <td>-1.357595</td>\n",
       "      <td>-1.051980</td>\n",
       "      <td>-1.163242</td>\n",
       "      <td>1.359299</td>\n",
       "      <td>-0.878143</td>\n",
       "      <td>-0.547154</td>\n",
       "      <td>0.439274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.972495</td>\n",
       "      <td>-1.184871</td>\n",
       "      <td>-1.100264</td>\n",
       "      <td>1.110875</td>\n",
       "      <td>1.479862</td>\n",
       "      <td>1.612517</td>\n",
       "      <td>2.388191</td>\n",
       "      <td>-1.161401</td>\n",
       "      <td>-1.084497</td>\n",
       "      <td>-0.937716</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.303650</td>\n",
       "      <td>2.024994</td>\n",
       "      <td>-1.168787</td>\n",
       "      <td>-1.247598</td>\n",
       "      <td>-1.229762</td>\n",
       "      <td>-1.181903</td>\n",
       "      <td>1.037983</td>\n",
       "      <td>-0.832998</td>\n",
       "      <td>-0.529270</td>\n",
       "      <td>0.321560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.528387</td>\n",
       "      <td>-1.134755</td>\n",
       "      <td>-1.056973</td>\n",
       "      <td>0.950950</td>\n",
       "      <td>-0.029304</td>\n",
       "      <td>2.967266</td>\n",
       "      <td>2.363688</td>\n",
       "      <td>-1.269510</td>\n",
       "      <td>-1.100845</td>\n",
       "      <td>-1.033805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.259354</td>\n",
       "      <td>2.911707</td>\n",
       "      <td>-1.223181</td>\n",
       "      <td>-1.254308</td>\n",
       "      <td>-1.104728</td>\n",
       "      <td>-1.007858</td>\n",
       "      <td>2.589265</td>\n",
       "      <td>-1.009226</td>\n",
       "      <td>-0.475952</td>\n",
       "      <td>0.105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.324637</td>\n",
       "      <td>-1.389314</td>\n",
       "      <td>-1.273155</td>\n",
       "      <td>0.844723</td>\n",
       "      <td>0.462561</td>\n",
       "      <td>1.727518</td>\n",
       "      <td>5.864649</td>\n",
       "      <td>-1.410896</td>\n",
       "      <td>-1.246113</td>\n",
       "      <td>-0.983316</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.540234</td>\n",
       "      <td>2.300212</td>\n",
       "      <td>-1.383186</td>\n",
       "      <td>-1.410981</td>\n",
       "      <td>-0.821116</td>\n",
       "      <td>-1.297357</td>\n",
       "      <td>1.475099</td>\n",
       "      <td>-0.581217</td>\n",
       "      <td>-0.870430</td>\n",
       "      <td>0.198977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.042343</td>\n",
       "      <td>-1.467812</td>\n",
       "      <td>-1.262910</td>\n",
       "      <td>0.782575</td>\n",
       "      <td>0.860313</td>\n",
       "      <td>2.148421</td>\n",
       "      <td>4.348184</td>\n",
       "      <td>-0.915376</td>\n",
       "      <td>-1.276812</td>\n",
       "      <td>-0.902049</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.478694</td>\n",
       "      <td>2.156381</td>\n",
       "      <td>-1.357730</td>\n",
       "      <td>-1.486966</td>\n",
       "      <td>-1.311104</td>\n",
       "      <td>-1.390450</td>\n",
       "      <td>0.915915</td>\n",
       "      <td>-1.091119</td>\n",
       "      <td>-0.828138</td>\n",
       "      <td>0.108991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.373409 -1.285452 -1.181257  0.866390  0.147068  1.610274  5.513695   \n",
       "1 -0.972495 -1.184871 -1.100264  1.110875  1.479862  1.612517  2.388191   \n",
       "2 -0.528387 -1.134755 -1.056973  0.950950 -0.029304  2.967266  2.363688   \n",
       "3 -0.324637 -1.389314 -1.273155  0.844723  0.462561  1.727518  5.864649   \n",
       "4 -1.042343 -1.467812 -1.262910  0.782575  0.860313  2.148421  4.348184   \n",
       "\n",
       "        7         8         9    ...       130       131       132       133  \\\n",
       "0 -1.326972 -1.120104 -0.849544  ... -1.457051  2.210021 -1.219851 -1.357595   \n",
       "1 -1.161401 -1.084497 -0.937716  ... -1.303650  2.024994 -1.168787 -1.247598   \n",
       "2 -1.269510 -1.100845 -1.033805  ... -1.259354  2.911707 -1.223181 -1.254308   \n",
       "3 -1.410896 -1.246113 -0.983316  ... -1.540234  2.300212 -1.383186 -1.410981   \n",
       "4 -0.915376 -1.276812 -0.902049  ... -1.478694  2.156381 -1.357730 -1.486966   \n",
       "\n",
       "        134       135       136       137       138       139  \n",
       "0 -1.051980 -1.163242  1.359299 -0.878143 -0.547154  0.439274  \n",
       "1 -1.229762 -1.181903  1.037983 -0.832998 -0.529270  0.321560  \n",
       "2 -1.104728 -1.007858  2.589265 -1.009226 -0.475952  0.105455  \n",
       "3 -0.821116 -1.297357  1.475099 -0.581217 -0.870430  0.198977  \n",
       "4 -1.311104 -1.390450  0.915915 -1.091119 -0.828138  0.108991  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model #16 prediction on private test data\n",
    "# model #16: cite_mlp_corr_svd_128_flg_donor_val_30\n",
    "\n",
    "pred_private = get_pred('cite_mlp_corr_svd_128_flg_donor_val_30', private_X_test_svd)\n",
    "print(pred_private.shape)\n",
    "pred_private.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb3f29a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.875269</td>\n",
       "      <td>1.072714</td>\n",
       "      <td>0.349734</td>\n",
       "      <td>2.983766</td>\n",
       "      <td>3.551555</td>\n",
       "      <td>10.595364</td>\n",
       "      <td>17.327169</td>\n",
       "      <td>-0.144145</td>\n",
       "      <td>1.184181</td>\n",
       "      <td>3.041042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191373</td>\n",
       "      <td>11.086969</td>\n",
       "      <td>0.300469</td>\n",
       "      <td>0.805080</td>\n",
       "      <td>-0.291616</td>\n",
       "      <td>1.635413</td>\n",
       "      <td>7.509739</td>\n",
       "      <td>6.417010</td>\n",
       "      <td>-0.497571</td>\n",
       "      <td>3.759374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.331542</td>\n",
       "      <td>0.871559</td>\n",
       "      <td>0.484194</td>\n",
       "      <td>-0.784014</td>\n",
       "      <td>4.173365</td>\n",
       "      <td>5.242312</td>\n",
       "      <td>3.511576</td>\n",
       "      <td>1.208851</td>\n",
       "      <td>0.547345</td>\n",
       "      <td>-1.133742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310369</td>\n",
       "      <td>7.197814</td>\n",
       "      <td>-0.425260</td>\n",
       "      <td>-0.154081</td>\n",
       "      <td>-0.124246</td>\n",
       "      <td>-0.106671</td>\n",
       "      <td>3.812726</td>\n",
       "      <td>-0.155547</td>\n",
       "      <td>1.029169</td>\n",
       "      <td>3.598739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.211242</td>\n",
       "      <td>0.736260</td>\n",
       "      <td>1.096481</td>\n",
       "      <td>1.892444</td>\n",
       "      <td>1.585475</td>\n",
       "      <td>3.669683</td>\n",
       "      <td>2.232614</td>\n",
       "      <td>-1.042255</td>\n",
       "      <td>0.724554</td>\n",
       "      <td>0.833527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204551</td>\n",
       "      <td>8.529263</td>\n",
       "      <td>1.732819</td>\n",
       "      <td>0.772459</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>0.747416</td>\n",
       "      <td>1.233450</td>\n",
       "      <td>0.753507</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>1.859444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.110175</td>\n",
       "      <td>1.230138</td>\n",
       "      <td>0.494763</td>\n",
       "      <td>2.138238</td>\n",
       "      <td>4.675085</td>\n",
       "      <td>7.903718</td>\n",
       "      <td>12.580855</td>\n",
       "      <td>1.051892</td>\n",
       "      <td>2.035204</td>\n",
       "      <td>-0.039051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094660</td>\n",
       "      <td>6.203796</td>\n",
       "      <td>1.358820</td>\n",
       "      <td>0.798049</td>\n",
       "      <td>-0.224005</td>\n",
       "      <td>0.375061</td>\n",
       "      <td>6.693098</td>\n",
       "      <td>2.614395</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>3.432812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259446</td>\n",
       "      <td>0.755755</td>\n",
       "      <td>1.337115</td>\n",
       "      <td>0.549588</td>\n",
       "      <td>1.717137</td>\n",
       "      <td>3.052538</td>\n",
       "      <td>3.472257</td>\n",
       "      <td>0.744798</td>\n",
       "      <td>0.519145</td>\n",
       "      <td>0.829422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103484</td>\n",
       "      <td>1.742605</td>\n",
       "      <td>0.397716</td>\n",
       "      <td>-0.141199</td>\n",
       "      <td>0.273376</td>\n",
       "      <td>1.399987</td>\n",
       "      <td>1.973929</td>\n",
       "      <td>-0.221435</td>\n",
       "      <td>1.301512</td>\n",
       "      <td>4.906563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4          5          6    \\\n",
       "0  8.875269  1.072714  0.349734  2.983766  3.551555  10.595364  17.327169   \n",
       "1 -0.331542  0.871559  0.484194 -0.784014  4.173365   5.242312   3.511576   \n",
       "2 -0.211242  0.736260  1.096481  1.892444  1.585475   3.669683   2.232614   \n",
       "3  4.110175  1.230138  0.494763  2.138238  4.675085   7.903718  12.580855   \n",
       "4  0.259446  0.755755  1.337115  0.549588  1.717137   3.052538   3.472257   \n",
       "\n",
       "        7         8         9    ...       130        131       132       133  \\\n",
       "0 -0.144145  1.184181  3.041042  ...  0.191373  11.086969  0.300469  0.805080   \n",
       "1  1.208851  0.547345 -1.133742  ...  0.310369   7.197814 -0.425260 -0.154081   \n",
       "2 -1.042255  0.724554  0.833527  ... -0.204551   8.529263  1.732819  0.772459   \n",
       "3  1.051892  2.035204 -0.039051  ... -0.094660   6.203796  1.358820  0.798049   \n",
       "4  0.744798  0.519145  0.829422  ... -0.103484   1.742605  0.397716 -0.141199   \n",
       "\n",
       "        134       135       136       137       138       139  \n",
       "0 -0.291616  1.635413  7.509739  6.417010 -0.497571  3.759374  \n",
       "1 -0.124246 -0.106671  3.812726 -0.155547  1.029169  3.598739  \n",
       "2 -0.043307  0.747416  1.233450  0.753507  0.735868  1.859444  \n",
       "3 -0.224005  0.375061  6.693098  2.614395  0.998246  3.432812  \n",
       "4  0.273376  1.399987  1.973929 -0.221435  1.301512  4.906563  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(private_test_target.X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95563b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(private_test_target_raw.X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "329971aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377997</td>\n",
       "      <td>0.471136</td>\n",
       "      <td>0.471176</td>\n",
       "      <td>0.455572</td>\n",
       "      <td>0.419737</td>\n",
       "      <td>0.503092</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.447481</td>\n",
       "      <td>0.481467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469957</td>\n",
       "      <td>0.475168</td>\n",
       "      <td>0.414638</td>\n",
       "      <td>0.498124</td>\n",
       "      <td>0.378822</td>\n",
       "      <td>0.457209</td>\n",
       "      <td>0.426564</td>\n",
       "      <td>0.521188</td>\n",
       "      <td>0.247760</td>\n",
       "      <td>0.381557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.377997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753910</td>\n",
       "      <td>0.575545</td>\n",
       "      <td>0.730001</td>\n",
       "      <td>0.682145</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>0.731498</td>\n",
       "      <td>0.735108</td>\n",
       "      <td>0.603452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678141</td>\n",
       "      <td>0.488619</td>\n",
       "      <td>0.638897</td>\n",
       "      <td>0.717295</td>\n",
       "      <td>0.301199</td>\n",
       "      <td>0.739038</td>\n",
       "      <td>0.443246</td>\n",
       "      <td>0.449856</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.664120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.471136</td>\n",
       "      <td>0.753910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619712</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.189060</td>\n",
       "      <td>0.663296</td>\n",
       "      <td>0.817229</td>\n",
       "      <td>0.682377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760124</td>\n",
       "      <td>0.604696</td>\n",
       "      <td>0.741847</td>\n",
       "      <td>0.794830</td>\n",
       "      <td>0.379428</td>\n",
       "      <td>0.789190</td>\n",
       "      <td>0.577407</td>\n",
       "      <td>0.543777</td>\n",
       "      <td>0.620387</td>\n",
       "      <td>0.648485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.471176</td>\n",
       "      <td>0.575545</td>\n",
       "      <td>0.619712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.746426</td>\n",
       "      <td>0.628963</td>\n",
       "      <td>0.328407</td>\n",
       "      <td>0.524641</td>\n",
       "      <td>0.591297</td>\n",
       "      <td>0.541863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561724</td>\n",
       "      <td>0.552039</td>\n",
       "      <td>0.525559</td>\n",
       "      <td>0.600623</td>\n",
       "      <td>0.347649</td>\n",
       "      <td>0.597710</td>\n",
       "      <td>0.489787</td>\n",
       "      <td>0.468808</td>\n",
       "      <td>0.387718</td>\n",
       "      <td>0.592743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.455572</td>\n",
       "      <td>0.730001</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>0.746426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763082</td>\n",
       "      <td>0.275992</td>\n",
       "      <td>0.682664</td>\n",
       "      <td>0.656426</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647981</td>\n",
       "      <td>0.535339</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.682793</td>\n",
       "      <td>0.361647</td>\n",
       "      <td>0.683325</td>\n",
       "      <td>0.384012</td>\n",
       "      <td>0.486183</td>\n",
       "      <td>0.319870</td>\n",
       "      <td>0.681190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.000000  0.377997  0.471136  0.471176  0.455572  0.419737  0.503092   \n",
       "1  0.377997  1.000000  0.753910  0.575545  0.730001  0.682145  0.067545   \n",
       "2  0.471136  0.753910  1.000000  0.619712  0.703420  0.616216  0.189060   \n",
       "3  0.471176  0.575545  0.619712  1.000000  0.746426  0.628963  0.328407   \n",
       "4  0.455572  0.730001  0.703420  0.746426  1.000000  0.763082  0.275992   \n",
       "\n",
       "        7         8         9    ...       130       131       132       133  \\\n",
       "0  0.296510  0.447481  0.481467  ...  0.469957  0.475168  0.414638  0.498124   \n",
       "1  0.731498  0.735108  0.603452  ...  0.678141  0.488619  0.638897  0.717295   \n",
       "2  0.663296  0.817229  0.682377  ...  0.760124  0.604696  0.741847  0.794830   \n",
       "3  0.524641  0.591297  0.541863  ...  0.561724  0.552039  0.525559  0.600623   \n",
       "4  0.682664  0.656426  0.593496  ...  0.647981  0.535339  0.550239  0.682793   \n",
       "\n",
       "        134       135       136       137       138       139  \n",
       "0  0.378822  0.457209  0.426564  0.521188  0.247760  0.381557  \n",
       "1  0.301199  0.739038  0.443246  0.449856  0.479300  0.664120  \n",
       "2  0.379428  0.789190  0.577407  0.543777  0.620387  0.648485  \n",
       "3  0.347649  0.597710  0.489787  0.468808  0.387718  0.592743  \n",
       "4  0.361647  0.683325  0.384012  0.486183  0.319870  0.681190  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(pred_private), pd.DataFrame(private_test_target.X)]).corr().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

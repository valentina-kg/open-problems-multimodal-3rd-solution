{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10483dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "!pip install --upgrade pip\n",
    "# !pip install --upgrade pandas\n",
    "!pip install tables   \n",
    "# necessary for pd.read_hdf()\n",
    "\n",
    "!pip install ipywidgets\n",
    "!pip install --upgrade jupyter\n",
    "!pip install IProgress\n",
    "!pip install catboost\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec82e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(output.stderr) # prints potential installation errors from cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb07152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f07041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26192e7",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05c209c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrz_path = '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/'\n",
    "\n",
    "model_path_for_now = '/dss/dsshome1/02/di93zoj/valentina/open-problems-multimodal-3rd-solution/'\n",
    "\n",
    "raw_path =  lrz_path + 'input/raw/'  # '../../../input/raw/'\n",
    "\n",
    "cite_target_path = lrz_path + 'input/target/cite/'   # '../../../input/target/cite/'\n",
    "cite_feature_path = lrz_path + 'input/features/cite/'   # '../../../input/features/cite/'\n",
    "cite_mlp_path = lrz_path + 'model/cite/mlp/'   # '../../../model/cite/mlp/'   # '../../../model/cite/mlp/'\n",
    "cite_cb_path = lrz_path + 'model/cite/cb/'   # '../../../model/cite/cb/'\n",
    "\n",
    "multi_target_path = lrz_path + 'input/target/multi/'   # '../../../input/target/multi/'\n",
    "multi_feature_path = lrz_path + 'input/features/multi/'   # '../../../input/features/multi/'\n",
    "multi_mlp_path = lrz_path + 'model/multi/mlp/'   # '../../../model/multi/mlp/'\n",
    "multi_cb_path = lrz_path + 'model/multi/cb/'   # '../../../model/multi/cb/'\n",
    "\n",
    "output_path = lrz_path + 'output/'   # '../../../output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac8787",
   "metadata": {},
   "source": [
    "### train datasets from model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309f87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "#                               training sets           test sets\n",
    "feature_dict['add_con_imp'] = ['X_add_con_imp.pickle', 'X_test_add_con_imp.pickle']\n",
    "feature_dict['last_v3'] = ['X_last_v3.pickle', 'X_test_last_v3.pickle']\n",
    "feature_dict['c_add_w2v_v1_mish'] = ['X_c_add_w2v_v1.pickle', 'X_test_c_add_w2v_v1.pickle']\n",
    "feature_dict['c_add_w2v_v1'] = ['X_c_add_w2v_v1.pickle', 'X_test_c_add_w2v_v1.pickle']\n",
    "feature_dict['c_add_84_v1'] = ['X_c_add_84_v1.pickle', 'X_test_c_add_84_v1.pickle']\n",
    "feature_dict['c_add_120_v1'] = ['X_c_add_v1.pickle', 'X_test_c_add_v1.pickle']\n",
    "\n",
    "feature_dict['w2v_cell'] = ['X_feature_w2v_cell.pickle', 'X_test_feature_w2v_cell.pickle']\n",
    "feature_dict['best_cell_120'] = ['X_best_cell_128_120.pickle', 'X_test_best_cell_128_120.pickle']\n",
    "feature_dict['cluster_cell'] = ['X_cluster_cell_128.pickle', 'X_test_cluster_cell_128.pickle']\n",
    "\n",
    "feature_dict['w2v_128'] = ['X_feature_w2v.pickle', 'X_test_feature_w2v.pickle']\n",
    "feature_dict['imp_w2v_128'] = ['X_feature_imp_w2v.pickle', 'X_test_feature_imp_w2v.pickle']\n",
    "feature_dict['snorm'] = ['X_feature_snorm.pickle', 'X_test_feature_snorm.pickle']\n",
    "feature_dict['best_128'] = ['X_best_128.pickle', 'X_test_best_128.pickle']\n",
    "feature_dict['best_64'] = ['X_best_64.pickle', 'X_test_best_64.pickle']\n",
    "feature_dict['cluster_128'] = ['X_cluster_128.pickle', 'X_test_cluster_128.pickle']\n",
    "feature_dict['cluster_64'] = ['X_cluster_64.pickle', 'X_test_cluster_64.pickle']\n",
    "feature_dict['svd_128'] = ['X_svd_128.pickle', 'X_test_svd_128.pickle']   # model #16\n",
    "feature_dict['svd_64'] = ['X_svd_64.pickle', 'X_test_svd_64.pickle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547d40e",
   "metadata": {},
   "source": [
    "## Cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3f14f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cite_mlp_corr_add_con_imp_flg_donor_val_50',\n",
       " 'cite_mlp_corr_c_add_84_v1_flg_donor_val_47',\n",
       " 'cite_mlp_corr_c_add_120_v1_flg_donor_val_63',\n",
       " '.ipynb_checkpoints',\n",
       " 'cite_mlp_corr_snorm_flg_donor_val_39',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_mish_flg_donor_val_66',\n",
       " 'cite_mlp_corr_cluster_128_flg_donor_val_51',\n",
       " 'cite_mlp_corr_svd_128_flg_donor_val_30',\n",
       " 'cite_mlp_corr_w2v_cell_flg_donor_val_51',\n",
       " 'cite_mlp_corr_cluster_64_flg_donor_val_57',\n",
       " 'cite_mlp_corr_w2v_128_flg_donor_val_42',\n",
       " 'cite_mlp_corr_cluster_cell_flg_donor_val_64',\n",
       " 'cite_mlp_corr_imp_w2v_128_flg_donor_val_38',\n",
       " 'cite_mlp_corr_best_cell_120_flg_donor_val_51',\n",
       " 'cite_mlp_corr_best_128_flg_donor_val_45',\n",
       " 'cite_mlp_corr_svd_64_flg_donor_val_38',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_flg_donor_val_66',\n",
       " 'cite_mlp_corr_best_64_flg_donor_val_50',\n",
       " '.gitkeep',\n",
       " 'cite_mlp_corr_last_v3_flg_donor_val_55']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model name\n",
    "mlp_model_path = os.listdir(cite_mlp_path)\n",
    "mlp_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb09944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_name = [\n",
    "    'corr_add_con_imp',\n",
    "    'corr_last_v3', \n",
    "    'corr_c_add_w2v_v1_mish_flg',\n",
    "    'corr_c_add_w2v_v1_flg',\n",
    "    'corr_c_add_84_v1',\n",
    "    'corr_c_add_120_v1',\n",
    "    'corr_w2v_cell_flg',\n",
    "    'corr_best_cell_120',\n",
    "    'corr_cluster_cell',\n",
    "    'corr_w2v_128',\n",
    "    'corr_imp_w2v_128',\n",
    "    'corr_snorm',\n",
    "    'corr_best_128',\n",
    "    'corr_best_64',\n",
    "    'corr_cluster_128',\n",
    "    'corr_cluster_64',\n",
    "    'corr_svd_128',\n",
    "    'corr_svd_64',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7653a277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cite_mlp_corr_add_con_imp_flg_donor_val_50',\n",
       " 'cite_mlp_corr_last_v3_flg_donor_val_55',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_mish_flg_donor_val_66',\n",
       " 'cite_mlp_corr_c_add_w2v_v1_flg_donor_val_66',\n",
       " 'cite_mlp_corr_c_add_84_v1_flg_donor_val_47',\n",
       " 'cite_mlp_corr_c_add_120_v1_flg_donor_val_63',\n",
       " 'cite_mlp_corr_w2v_cell_flg_donor_val_51',\n",
       " 'cite_mlp_corr_best_cell_120_flg_donor_val_51',\n",
       " 'cite_mlp_corr_cluster_cell_flg_donor_val_64',\n",
       " 'cite_mlp_corr_w2v_128_flg_donor_val_42',\n",
       " 'cite_mlp_corr_imp_w2v_128_flg_donor_val_38',\n",
       " 'cite_mlp_corr_snorm_flg_donor_val_39',\n",
       " 'cite_mlp_corr_best_128_flg_donor_val_45',\n",
       " 'cite_mlp_corr_best_64_flg_donor_val_50',\n",
       " 'cite_mlp_corr_cluster_128_flg_donor_val_51',\n",
       " 'cite_mlp_corr_cluster_64_flg_donor_val_57',\n",
       " 'cite_mlp_corr_svd_128_flg_donor_val_30',\n",
       " 'cite_mlp_corr_svd_64_flg_donor_val_38']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_list = []\n",
    "\n",
    "for i in mlp_model_name:\n",
    "    for num, j in enumerate(os.listdir(cite_mlp_path)):\n",
    "        if i in j:\n",
    "            model_name_list.append(j)\n",
    "\n",
    "len(model_name_list)\n",
    "model_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835d75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [1, 0.3, 1, 1, 1, 1, 1, 1, 1, 0.8, 0.8, 0.8, 0.8, 0.5, 0.5, 0.5, 1, 1, 2, 2]\n",
    "weight_sum = np.array(weight).sum()\n",
    "weight_sum\n",
    "\n",
    "# dict:            model name          input-feature-cite file:    weight\n",
    "model_feat_dict = {model_name_list[0]:['X_test_add_con_imp.pickle', 1],\n",
    "                   model_name_list[1]:['X_test_last_v3.pickle', 0.3],\n",
    "                   model_name_list[2]:['X_test_c_add_w2v_v1.pickle', 1],\n",
    "                   model_name_list[3]:['X_test_c_add_w2v_v1.pickle', 1],\n",
    "                   model_name_list[4]:['X_test_c_add_84_v1.pickle', 1],\n",
    "                   model_name_list[5]:['X_test_c_add_v1.pickle', 1],\n",
    "                   \n",
    "                   model_name_list[6]:['X_test_feature_w2v_cell.pickle', 1],\n",
    "                   model_name_list[7]:['X_test_best_cell_128_120.pickle', 1],\n",
    "                   model_name_list[8]:['X_test_cluster_cell_128.pickle', 1],\n",
    "                   \n",
    "                   model_name_list[9]:['X_test_feature_w2v.pickle', 0.8],\n",
    "                   model_name_list[10]:['X_test_feature_imp_w2v.pickle',0.8],\n",
    "                   model_name_list[11]:['X_test_feature_snorm.pickle', 0.8],\n",
    "                   model_name_list[12]:['X_test_best_128.pickle', 0.8],\n",
    "                   model_name_list[13]:['X_test_best_64.pickle', 0.5],\n",
    "                   model_name_list[14]:['X_test_cluster_128.pickle', 0.5],\n",
    "                   model_name_list[15]:['X_test_cluster_64.pickle', 0.5],\n",
    "                   model_name_list[16]:['X_test_svd_128.pickle', 1],\n",
    "                   model_name_list[17]:['X_test_svd_64.pickle', 1],\n",
    "                   \n",
    "                   'best_128':['X_test_best_128.pickle', 2],\n",
    "                   'best_64':['X_test_best_64.pickle', 2],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1fdbeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_add_con_imp.pickle\n",
      "(48203, 925)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_last_v3.pickle\n",
      "(48203, 843)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_c_add_w2v_v1.pickle\n",
      "(48203, 843)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_c_add_w2v_v1.pickle\n",
      "(48203, 843)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_c_add_84_v1.pickle\n",
      "(48203, 667)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_c_add_v1.pickle\n",
      "(48203, 811)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_feature_w2v_cell.pickle\n",
      "(48203, 752)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_best_cell_128_120.pickle\n",
      "(48203, 900)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_cluster_cell_128.pickle\n",
      "(48203, 556)\n",
      "pickle data was truncated\n",
      "UnpicklingError:  cite_mlp_corr_w2v_128_flg_donor_val_42\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_feature_imp_w2v.pickle\n",
      "(48203, 645)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_feature_snorm.pickle\n",
      "(48203, 745)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_best_128.pickle\n",
      "(48203, 629)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_best_64.pickle\n",
      "(48203, 565)\n",
      "pickle data was truncated\n",
      "UnpicklingError:  cite_mlp_corr_cluster_128_flg_donor_val_51\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_cluster_64.pickle\n",
      "(48203, 401)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_svd_128.pickle\n",
      "(48203, 212)\n",
      "/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/open-problems-multimodal-3rd-solution/input/features/cite/X_test_svd_64.pickle\n",
      "(48203, 148)\n"
     ]
    }
   ],
   "source": [
    "# new\n",
    "for i in model_name_list:\n",
    "    #i = 'cite_mlp_corr_snorm_flg_donor_val_39'\n",
    "    try:\n",
    "        test_file = model_feat_dict[i][0]\n",
    "        X_test = pd.read_pickle(cite_feature_path  + test_file)   \n",
    "        print(cite_feature_path  + test_file)\n",
    "        print(X_test.shape)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('UnpicklingError: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8acb7d",
   "metadata": {},
   "source": [
    "### cite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(x):\n",
    "    x = np.array(x)\n",
    "    return (x - x.mean(1).reshape(-1, 1)) / x.std(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca48b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, feature, target):\n",
    "        \n",
    "        self.feature = feature\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        d = {\n",
    "            \"X\": self.feature[index],\n",
    "            \"y\" : self.target[index],\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9366795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteDataset_test(Dataset):\n",
    "    \n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        d = {\n",
    "            \"X\": self.feature[index]\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bb8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation_score_torch_faster(y_true, y_pred):\n",
    "    \"\"\"Compute the correlation between each rows of the y_true and y_pred tensors.\n",
    "    Compatible with backpropagation.\n",
    "    \"\"\"\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1)[:,None]\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:,None]\n",
    "    cov_tp = torch.sum(y_true_centered*y_pred_centered, dim=1)/(y_true.shape[1]-1)\n",
    "    var_t = torch.sum(y_true_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    var_p = torch.sum(y_pred_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    return cov_tp/torch.sqrt(var_t*var_p)\n",
    "\n",
    "def correl_loss(pred, tgt):\n",
    "    \"\"\"Loss for directly optimizing the correlation.\n",
    "    \"\"\"\n",
    "    return -torch.mean(partial_correlation_score_torch_faster(tgt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2229f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num):\n",
    "        super(CiteModel, self).__init__()\n",
    "        \n",
    "        self.layer_seq_256 = nn.Sequential(nn.Linear(feature_num, 256),\n",
    "                                           nn.Linear(256, 128),\n",
    "                                       nn.LayerNorm(128),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        self.layer_seq_64 = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Linear(64, 32),\n",
    "                                       nn.LayerNorm(32),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        self.layer_seq_8 = nn.Sequential(nn.Linear(32, 16),\n",
    "                                         nn.Linear(16, 8),\n",
    "                                       nn.LayerNorm(8),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        \n",
    "        self.head = nn.Linear(128 + 32 + 8, 140)\n",
    "                   \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        ##\n",
    "#         if isinstance(X, np.ndarray):\n",
    "#             X = torch.from_numpy(X)\n",
    "#         X = X.to(device)  # Move the input to the appropriate device if necessary\n",
    "        ##\n",
    "    \n",
    "        X_256 = self.layer_seq_256(X)\n",
    "        X_64 = self.layer_seq_64(X_256)\n",
    "        X_8 = self.layer_seq_8(X_64)\n",
    "        \n",
    "        X = torch.cat([X_256, X_64, X_8], axis = 1)\n",
    "        out = self.head(X)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d75d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteModel_mish(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num):\n",
    "        super(CiteModel_mish, self).__init__()\n",
    "        \n",
    "        self.layer_seq_256 = nn.Sequential(nn.Linear(feature_num, 256),\n",
    "                                           nn.Linear(256, 128),\n",
    "                                       nn.LayerNorm(128),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        self.layer_seq_64 = nn.Sequential(nn.Linear(128, 64),\n",
    "                                       nn.Linear(64, 32),\n",
    "                                       nn.LayerNorm(32),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        self.layer_seq_8 = nn.Sequential(nn.Linear(32, 16),\n",
    "                                         nn.Linear(16, 8),\n",
    "                                       nn.LayerNorm(8),\n",
    "                                       nn.Mish(),\n",
    "                                      )\n",
    "        \n",
    "        self.head = nn.Linear(128 + 32 + 8, 140)\n",
    "                   \n",
    "    def forward(self, X, y=None):\n",
    "    \n",
    "        X_256 = self.layer_seq_256(X)\n",
    "        X_64 = self.layer_seq_64(X_256)\n",
    "        X_8 = self.layer_seq_8(X_64)\n",
    "        \n",
    "        X = torch.cat([X_256, X_64, X_8], axis = 1)\n",
    "        out = self.head(X)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55c58cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, loader, epoch):\n",
    "    \n",
    "    losses, lrs = [], []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with tqdm(total=len(loader),unit=\"batch\") as pbar:\n",
    "        pbar.set_description(f\"Epoch{epoch}\")\n",
    "        \n",
    "        for d in loader:\n",
    "            X = d['X'].to(device)\n",
    "            y = d['y'].to(device)\n",
    "            \n",
    "            logits = model(X)\n",
    "            loss = correl_loss(logits, y)\n",
    "            #loss = torch.sqrt(loss_fn(logits, y))\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix({\"loss\":loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9025ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop(model, loader, y_val):\n",
    "    \n",
    "    model.eval()\n",
    "    partial_correlation_scores = []\n",
    "    oof_pred = []\n",
    "    \n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            val_X = d['X'].to(device).float()\n",
    "            val_y = d['y'].to(device)\n",
    "            logits = model(val_X)\n",
    "            oof_pred.append(logits)\n",
    "    \n",
    "    #print(torch.cat(oof_pred).shape, torch.cat(oof_pred).detach().cpu().numpy().shape)\n",
    "    cor = partial_correlation_score_torch_faster(torch.tensor(y_val).to(device), torch.cat(oof_pred))\n",
    "    cor = cor.mean().item()\n",
    "    logits = torch.cat(oof_pred).detach().cpu().numpy()\n",
    "    \n",
    "    return logits, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f443927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    predicts=[]\n",
    "\n",
    "    for d in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            X = d['X'].to(device)\n",
    "            logits = model(X)\n",
    "            predicts.append(logits.detach().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d01b13",
   "metadata": {},
   "source": [
    "### pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c97b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cite_mlp_corr_add_con_imp_flg_donor_val_50\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_last_v3_flg_donor_val_55\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_c_add_w2v_v1_mish_flg_donor_val_66\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_c_add_w2v_v1_flg_donor_val_66\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_c_add_84_v1_flg_donor_val_47\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_c_add_120_v1_flg_donor_val_63\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_w2v_cell_flg_donor_val_51\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_best_cell_120_flg_donor_val_51\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_cluster_cell_flg_donor_val_64\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_w2v_128_flg_donor_val_42\n",
      "pickle data was truncated\n",
      "cite_mlp_corr_imp_w2v_128_flg_donor_val_38\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_snorm_flg_donor_val_39\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_best_128_flg_donor_val_45\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_best_64_flg_donor_val_50\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_cluster_128_flg_donor_val_51\n",
      "pickle data was truncated\n",
      "cite_mlp_corr_cluster_64_flg_donor_val_57\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_svd_128_flg_donor_val_30\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n",
      "cite_mlp_corr_svd_64_flg_donor_val_38\n",
      "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc452069f6da49b6a3c105b8a152a7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc72aa876d4743e3963cd657b47a085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = np.zeros([48203, 140])\n",
    "\n",
    "for num, i in enumerate(model_feat_dict.keys()):\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    if 'mlp' in i:\n",
    "\n",
    "        try:\n",
    "            test_file = model_feat_dict[i][0]\n",
    "            test_weight = model_feat_dict[i][1]\n",
    "            X_test = pd.read_pickle(cite_feature_path  + test_file)   \n",
    "            # print(cite_feature_path  + test_file)\n",
    "            X_test = np.array(X_test)\n",
    "            feature_dims = X_test.shape[1]\n",
    "\n",
    "            test_ds = CiteDataset_test(X_test)\n",
    "            test_dataloader = DataLoader(test_ds, batch_size=128, pin_memory=True, \n",
    "                                         shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "            if 'mish' in i:\n",
    "                model = CiteModel_mish(feature_dims)\n",
    "            else:\n",
    "                model = CiteModel(feature_dims)\n",
    "\n",
    "            model = model.to(device)\n",
    "            model.load_state_dict(torch.load(f'{cite_mlp_path}/{i}'))\n",
    "\n",
    "            result = test_loop(model, test_dataloader).astype(np.float32)\n",
    "            result = std(result) * test_weight / weight_sum\n",
    "            pred += result\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e: \n",
    "            print(i)\n",
    "            print(e)             # TODOOOOOOOOOOOOOO\n",
    "        \n",
    "    else:\n",
    "        test_file = model_feat_dict[i][0]\n",
    "        test_weight = model_feat_dict[i][1]\n",
    "        X_test = pd.read_pickle(cite_feature_path  + test_file)\n",
    "        \n",
    "        cb_pred = np.zeros([48203, 140])\n",
    "        \n",
    "        for t in tqdm(range(140)): \n",
    "            cb_model_path = [j for j in os.listdir(cite_cb_path) if f'cb_{t}_{i}' in j][0]\n",
    "            cb = pickle.load(open(cite_cb_path + cb_model_path, 'rb'))\n",
    "            cb_pred[:,t] = cb.predict(X_test)\n",
    "            \n",
    "        cb_pred = cb_pred.astype(np.float32)\n",
    "        pred += std(cb_pred) * test_weight / weight_sum\n",
    "        \n",
    "        #del cb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2ac31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.076877</td>\n",
       "      <td>-0.107536</td>\n",
       "      <td>-0.081412</td>\n",
       "      <td>0.235340</td>\n",
       "      <td>0.212383</td>\n",
       "      <td>0.508778</td>\n",
       "      <td>0.714871</td>\n",
       "      <td>-0.111080</td>\n",
       "      <td>-0.094659</td>\n",
       "      <td>-0.073315</td>\n",
       "      <td>-0.116281</td>\n",
       "      <td>-0.108862</td>\n",
       "      <td>-0.097573</td>\n",
       "      <td>-0.104481</td>\n",
       "      <td>0.483716</td>\n",
       "      <td>-0.079324</td>\n",
       "      <td>0.426531</td>\n",
       "      <td>0.302832</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>-0.107653</td>\n",
       "      <td>-0.082257</td>\n",
       "      <td>0.090119</td>\n",
       "      <td>-0.112168</td>\n",
       "      <td>-0.095676</td>\n",
       "      <td>0.687361</td>\n",
       "      <td>-0.113379</td>\n",
       "      <td>-0.115403</td>\n",
       "      <td>-0.108167</td>\n",
       "      <td>-0.117696</td>\n",
       "      <td>-0.108534</td>\n",
       "      <td>-0.111692</td>\n",
       "      <td>-0.104854</td>\n",
       "      <td>-0.105364</td>\n",
       "      <td>-0.105601</td>\n",
       "      <td>-0.103499</td>\n",
       "      <td>-0.111801</td>\n",
       "      <td>-0.088663</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>-0.095526</td>\n",
       "      <td>-0.093547</td>\n",
       "      <td>-0.110812</td>\n",
       "      <td>-0.105767</td>\n",
       "      <td>-0.112340</td>\n",
       "      <td>-0.092821</td>\n",
       "      <td>-0.116772</td>\n",
       "      <td>-0.105450</td>\n",
       "      <td>-0.081068</td>\n",
       "      <td>-0.082683</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>-0.109826</td>\n",
       "      <td>-0.104239</td>\n",
       "      <td>-0.105984</td>\n",
       "      <td>-0.094111</td>\n",
       "      <td>-0.115045</td>\n",
       "      <td>-0.020432</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>-0.105935</td>\n",
       "      <td>0.272084</td>\n",
       "      <td>-0.105307</td>\n",
       "      <td>-0.106435</td>\n",
       "      <td>-0.111940</td>\n",
       "      <td>-0.097159</td>\n",
       "      <td>-0.089999</td>\n",
       "      <td>-0.103777</td>\n",
       "      <td>-0.112325</td>\n",
       "      <td>-0.095288</td>\n",
       "      <td>-0.079516</td>\n",
       "      <td>-0.093184</td>\n",
       "      <td>0.072477</td>\n",
       "      <td>-0.109951</td>\n",
       "      <td>-0.110628</td>\n",
       "      <td>-0.118938</td>\n",
       "      <td>-0.107578</td>\n",
       "      <td>0.769669</td>\n",
       "      <td>-0.110701</td>\n",
       "      <td>0.448382</td>\n",
       "      <td>-0.105106</td>\n",
       "      <td>0.076690</td>\n",
       "      <td>-0.108419</td>\n",
       "      <td>-0.109099</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>-0.113356</td>\n",
       "      <td>-0.060216</td>\n",
       "      <td>-0.099415</td>\n",
       "      <td>-0.112301</td>\n",
       "      <td>-0.113646</td>\n",
       "      <td>-0.097630</td>\n",
       "      <td>-0.107329</td>\n",
       "      <td>-0.081830</td>\n",
       "      <td>-0.101124</td>\n",
       "      <td>-0.079099</td>\n",
       "      <td>-0.110585</td>\n",
       "      <td>-0.103025</td>\n",
       "      <td>-0.086625</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>-0.096740</td>\n",
       "      <td>-0.099395</td>\n",
       "      <td>0.420336</td>\n",
       "      <td>-0.103349</td>\n",
       "      <td>0.363882</td>\n",
       "      <td>0.214918</td>\n",
       "      <td>-0.115309</td>\n",
       "      <td>0.262475</td>\n",
       "      <td>-0.095930</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>-0.092371</td>\n",
       "      <td>0.312263</td>\n",
       "      <td>-0.070511</td>\n",
       "      <td>0.484739</td>\n",
       "      <td>-0.072997</td>\n",
       "      <td>-0.068368</td>\n",
       "      <td>0.107584</td>\n",
       "      <td>-0.087482</td>\n",
       "      <td>-0.107399</td>\n",
       "      <td>-0.112273</td>\n",
       "      <td>0.073439</td>\n",
       "      <td>-0.038099</td>\n",
       "      <td>-0.095309</td>\n",
       "      <td>-0.102715</td>\n",
       "      <td>0.572418</td>\n",
       "      <td>-0.099416</td>\n",
       "      <td>0.070323</td>\n",
       "      <td>-0.106917</td>\n",
       "      <td>-0.111302</td>\n",
       "      <td>-0.094307</td>\n",
       "      <td>-0.092461</td>\n",
       "      <td>-0.110055</td>\n",
       "      <td>-0.072320</td>\n",
       "      <td>-0.101630</td>\n",
       "      <td>-0.110362</td>\n",
       "      <td>-0.113396</td>\n",
       "      <td>0.400050</td>\n",
       "      <td>-0.090653</td>\n",
       "      <td>-0.115911</td>\n",
       "      <td>-0.114397</td>\n",
       "      <td>-0.078690</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>-0.056059</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.078415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.070980</td>\n",
       "      <td>-0.105483</td>\n",
       "      <td>-0.078728</td>\n",
       "      <td>0.238921</td>\n",
       "      <td>0.227432</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.868887</td>\n",
       "      <td>-0.106807</td>\n",
       "      <td>-0.090079</td>\n",
       "      <td>-0.077481</td>\n",
       "      <td>-0.116207</td>\n",
       "      <td>-0.105719</td>\n",
       "      <td>-0.089185</td>\n",
       "      <td>-0.101150</td>\n",
       "      <td>0.476003</td>\n",
       "      <td>-0.084783</td>\n",
       "      <td>0.395964</td>\n",
       "      <td>0.322830</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>-0.101084</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.063508</td>\n",
       "      <td>-0.109683</td>\n",
       "      <td>-0.085107</td>\n",
       "      <td>0.715892</td>\n",
       "      <td>-0.106425</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>-0.107168</td>\n",
       "      <td>-0.115465</td>\n",
       "      <td>-0.103104</td>\n",
       "      <td>-0.106192</td>\n",
       "      <td>-0.099530</td>\n",
       "      <td>-0.101643</td>\n",
       "      <td>-0.100825</td>\n",
       "      <td>-0.101097</td>\n",
       "      <td>-0.109965</td>\n",
       "      <td>-0.090401</td>\n",
       "      <td>0.831216</td>\n",
       "      <td>-0.089985</td>\n",
       "      <td>-0.093495</td>\n",
       "      <td>-0.107225</td>\n",
       "      <td>-0.103154</td>\n",
       "      <td>-0.108755</td>\n",
       "      <td>-0.103866</td>\n",
       "      <td>-0.113471</td>\n",
       "      <td>-0.102514</td>\n",
       "      <td>-0.077068</td>\n",
       "      <td>-0.083564</td>\n",
       "      <td>0.589613</td>\n",
       "      <td>-0.106611</td>\n",
       "      <td>-0.099559</td>\n",
       "      <td>-0.103316</td>\n",
       "      <td>-0.106078</td>\n",
       "      <td>-0.110636</td>\n",
       "      <td>-0.025642</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>-0.102003</td>\n",
       "      <td>0.180827</td>\n",
       "      <td>-0.103470</td>\n",
       "      <td>-0.103296</td>\n",
       "      <td>-0.104360</td>\n",
       "      <td>-0.095484</td>\n",
       "      <td>-0.088204</td>\n",
       "      <td>-0.098695</td>\n",
       "      <td>-0.108114</td>\n",
       "      <td>-0.093327</td>\n",
       "      <td>-0.078601</td>\n",
       "      <td>-0.091153</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>-0.108536</td>\n",
       "      <td>-0.103518</td>\n",
       "      <td>-0.112984</td>\n",
       "      <td>-0.104845</td>\n",
       "      <td>0.734899</td>\n",
       "      <td>-0.107256</td>\n",
       "      <td>0.403173</td>\n",
       "      <td>-0.101109</td>\n",
       "      <td>0.039439</td>\n",
       "      <td>-0.103329</td>\n",
       "      <td>-0.103254</td>\n",
       "      <td>0.018683</td>\n",
       "      <td>-0.110621</td>\n",
       "      <td>-0.056592</td>\n",
       "      <td>-0.095719</td>\n",
       "      <td>-0.110038</td>\n",
       "      <td>-0.108561</td>\n",
       "      <td>-0.095397</td>\n",
       "      <td>-0.102081</td>\n",
       "      <td>-0.078001</td>\n",
       "      <td>-0.104646</td>\n",
       "      <td>-0.079995</td>\n",
       "      <td>-0.105553</td>\n",
       "      <td>-0.101321</td>\n",
       "      <td>-0.083420</td>\n",
       "      <td>0.043079</td>\n",
       "      <td>-0.092405</td>\n",
       "      <td>-0.097549</td>\n",
       "      <td>0.416196</td>\n",
       "      <td>-0.102180</td>\n",
       "      <td>0.356702</td>\n",
       "      <td>0.219733</td>\n",
       "      <td>-0.110612</td>\n",
       "      <td>0.246460</td>\n",
       "      <td>-0.093005</td>\n",
       "      <td>-0.011302</td>\n",
       "      <td>-0.092891</td>\n",
       "      <td>0.270515</td>\n",
       "      <td>-0.075828</td>\n",
       "      <td>0.516748</td>\n",
       "      <td>-0.078225</td>\n",
       "      <td>-0.073941</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>-0.080481</td>\n",
       "      <td>-0.097804</td>\n",
       "      <td>-0.106804</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>-0.055215</td>\n",
       "      <td>-0.090355</td>\n",
       "      <td>-0.100601</td>\n",
       "      <td>0.594136</td>\n",
       "      <td>-0.099683</td>\n",
       "      <td>0.170995</td>\n",
       "      <td>-0.104191</td>\n",
       "      <td>-0.107329</td>\n",
       "      <td>-0.089034</td>\n",
       "      <td>-0.090868</td>\n",
       "      <td>-0.105538</td>\n",
       "      <td>-0.057181</td>\n",
       "      <td>-0.096199</td>\n",
       "      <td>-0.107437</td>\n",
       "      <td>-0.111155</td>\n",
       "      <td>0.405383</td>\n",
       "      <td>-0.086804</td>\n",
       "      <td>-0.110228</td>\n",
       "      <td>-0.110396</td>\n",
       "      <td>-0.084210</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>-0.076285</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.078346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.089150</td>\n",
       "      <td>-0.101540</td>\n",
       "      <td>-0.076435</td>\n",
       "      <td>0.338878</td>\n",
       "      <td>0.290691</td>\n",
       "      <td>0.323657</td>\n",
       "      <td>1.014777</td>\n",
       "      <td>-0.105268</td>\n",
       "      <td>-0.083472</td>\n",
       "      <td>-0.092418</td>\n",
       "      <td>-0.112782</td>\n",
       "      <td>-0.102729</td>\n",
       "      <td>-0.092453</td>\n",
       "      <td>-0.097368</td>\n",
       "      <td>0.790603</td>\n",
       "      <td>-0.063658</td>\n",
       "      <td>0.255258</td>\n",
       "      <td>0.259364</td>\n",
       "      <td>-0.007930</td>\n",
       "      <td>-0.099144</td>\n",
       "      <td>-0.079277</td>\n",
       "      <td>0.090837</td>\n",
       "      <td>-0.107481</td>\n",
       "      <td>-0.086070</td>\n",
       "      <td>0.681157</td>\n",
       "      <td>-0.103144</td>\n",
       "      <td>-0.109113</td>\n",
       "      <td>-0.100578</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>-0.096156</td>\n",
       "      <td>-0.102624</td>\n",
       "      <td>-0.092398</td>\n",
       "      <td>-0.097464</td>\n",
       "      <td>-0.097798</td>\n",
       "      <td>-0.095754</td>\n",
       "      <td>-0.104699</td>\n",
       "      <td>-0.085765</td>\n",
       "      <td>0.723438</td>\n",
       "      <td>-0.087787</td>\n",
       "      <td>-0.088923</td>\n",
       "      <td>-0.104203</td>\n",
       "      <td>-0.101058</td>\n",
       "      <td>-0.102143</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>-0.107397</td>\n",
       "      <td>-0.097045</td>\n",
       "      <td>-0.074226</td>\n",
       "      <td>-0.085142</td>\n",
       "      <td>0.200093</td>\n",
       "      <td>-0.104822</td>\n",
       "      <td>-0.092768</td>\n",
       "      <td>-0.099995</td>\n",
       "      <td>-0.080043</td>\n",
       "      <td>-0.107574</td>\n",
       "      <td>-0.019395</td>\n",
       "      <td>0.032191</td>\n",
       "      <td>-0.101605</td>\n",
       "      <td>0.177870</td>\n",
       "      <td>-0.100262</td>\n",
       "      <td>-0.098974</td>\n",
       "      <td>-0.051951</td>\n",
       "      <td>-0.086125</td>\n",
       "      <td>-0.083883</td>\n",
       "      <td>-0.091763</td>\n",
       "      <td>-0.102663</td>\n",
       "      <td>-0.089455</td>\n",
       "      <td>-0.072468</td>\n",
       "      <td>-0.088659</td>\n",
       "      <td>0.086627</td>\n",
       "      <td>-0.104616</td>\n",
       "      <td>-0.094370</td>\n",
       "      <td>-0.107391</td>\n",
       "      <td>-0.101939</td>\n",
       "      <td>0.314863</td>\n",
       "      <td>-0.100397</td>\n",
       "      <td>0.283052</td>\n",
       "      <td>-0.099730</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>-0.099223</td>\n",
       "      <td>-0.102801</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.103728</td>\n",
       "      <td>-0.036343</td>\n",
       "      <td>-0.091792</td>\n",
       "      <td>-0.109485</td>\n",
       "      <td>-0.104571</td>\n",
       "      <td>-0.091584</td>\n",
       "      <td>-0.099345</td>\n",
       "      <td>-0.069711</td>\n",
       "      <td>-0.100305</td>\n",
       "      <td>-0.074391</td>\n",
       "      <td>-0.102350</td>\n",
       "      <td>-0.090838</td>\n",
       "      <td>-0.080520</td>\n",
       "      <td>0.039465</td>\n",
       "      <td>-0.090182</td>\n",
       "      <td>-0.092018</td>\n",
       "      <td>0.526895</td>\n",
       "      <td>-0.098591</td>\n",
       "      <td>0.248171</td>\n",
       "      <td>0.198832</td>\n",
       "      <td>-0.108625</td>\n",
       "      <td>0.113670</td>\n",
       "      <td>-0.087849</td>\n",
       "      <td>0.206987</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.187265</td>\n",
       "      <td>-0.094517</td>\n",
       "      <td>0.585659</td>\n",
       "      <td>-0.078287</td>\n",
       "      <td>-0.062720</td>\n",
       "      <td>-0.034883</td>\n",
       "      <td>-0.078428</td>\n",
       "      <td>-0.089400</td>\n",
       "      <td>-0.101648</td>\n",
       "      <td>-0.047284</td>\n",
       "      <td>-0.077511</td>\n",
       "      <td>-0.092484</td>\n",
       "      <td>-0.060043</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>-0.095282</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.100147</td>\n",
       "      <td>-0.103879</td>\n",
       "      <td>-0.089214</td>\n",
       "      <td>-0.085673</td>\n",
       "      <td>-0.102612</td>\n",
       "      <td>-0.048358</td>\n",
       "      <td>-0.092067</td>\n",
       "      <td>-0.100856</td>\n",
       "      <td>-0.105592</td>\n",
       "      <td>0.657454</td>\n",
       "      <td>-0.085529</td>\n",
       "      <td>-0.106070</td>\n",
       "      <td>-0.105622</td>\n",
       "      <td>-0.088992</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>-0.068035</td>\n",
       "      <td>0.034081</td>\n",
       "      <td>0.154974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.128470</td>\n",
       "      <td>-0.113081</td>\n",
       "      <td>-0.063264</td>\n",
       "      <td>0.226137</td>\n",
       "      <td>0.264680</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>-0.138097</td>\n",
       "      <td>-0.083314</td>\n",
       "      <td>-0.076588</td>\n",
       "      <td>-0.114464</td>\n",
       "      <td>-0.140753</td>\n",
       "      <td>-0.099502</td>\n",
       "      <td>-0.109232</td>\n",
       "      <td>-0.089628</td>\n",
       "      <td>0.474046</td>\n",
       "      <td>-0.116054</td>\n",
       "      <td>0.243485</td>\n",
       "      <td>-0.126989</td>\n",
       "      <td>-0.007528</td>\n",
       "      <td>-0.024485</td>\n",
       "      <td>-0.085576</td>\n",
       "      <td>0.149265</td>\n",
       "      <td>-0.118302</td>\n",
       "      <td>-0.085031</td>\n",
       "      <td>0.411208</td>\n",
       "      <td>-0.119124</td>\n",
       "      <td>-0.130412</td>\n",
       "      <td>-0.142772</td>\n",
       "      <td>-0.121572</td>\n",
       "      <td>-0.107517</td>\n",
       "      <td>-0.112074</td>\n",
       "      <td>-0.087340</td>\n",
       "      <td>-0.113672</td>\n",
       "      <td>-0.113528</td>\n",
       "      <td>-0.107757</td>\n",
       "      <td>-0.124679</td>\n",
       "      <td>-0.049125</td>\n",
       "      <td>0.723409</td>\n",
       "      <td>-0.068823</td>\n",
       "      <td>-0.082963</td>\n",
       "      <td>-0.110676</td>\n",
       "      <td>-0.088543</td>\n",
       "      <td>-0.105773</td>\n",
       "      <td>0.603149</td>\n",
       "      <td>-0.146289</td>\n",
       "      <td>-0.110784</td>\n",
       "      <td>-0.024087</td>\n",
       "      <td>-0.115214</td>\n",
       "      <td>-0.053530</td>\n",
       "      <td>-0.118277</td>\n",
       "      <td>-0.074823</td>\n",
       "      <td>-0.102637</td>\n",
       "      <td>-0.066636</td>\n",
       "      <td>-0.129014</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>-0.050958</td>\n",
       "      <td>-0.101564</td>\n",
       "      <td>0.207655</td>\n",
       "      <td>-0.116863</td>\n",
       "      <td>-0.108798</td>\n",
       "      <td>-0.131065</td>\n",
       "      <td>-0.084988</td>\n",
       "      <td>-0.066779</td>\n",
       "      <td>-0.093403</td>\n",
       "      <td>-0.129859</td>\n",
       "      <td>-0.053791</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>-0.051470</td>\n",
       "      <td>0.141465</td>\n",
       "      <td>-0.130556</td>\n",
       "      <td>-0.115734</td>\n",
       "      <td>-0.124357</td>\n",
       "      <td>-0.109641</td>\n",
       "      <td>0.041874</td>\n",
       "      <td>-0.134306</td>\n",
       "      <td>0.284571</td>\n",
       "      <td>-0.099015</td>\n",
       "      <td>-0.003217</td>\n",
       "      <td>-0.113088</td>\n",
       "      <td>-0.116721</td>\n",
       "      <td>-0.081502</td>\n",
       "      <td>-0.142544</td>\n",
       "      <td>-0.060783</td>\n",
       "      <td>-0.077973</td>\n",
       "      <td>-0.138675</td>\n",
       "      <td>-0.134785</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>-0.119694</td>\n",
       "      <td>0.048126</td>\n",
       "      <td>0.725502</td>\n",
       "      <td>-0.036445</td>\n",
       "      <td>-0.120802</td>\n",
       "      <td>-0.100612</td>\n",
       "      <td>-0.033217</td>\n",
       "      <td>-0.031502</td>\n",
       "      <td>-0.067413</td>\n",
       "      <td>-0.078095</td>\n",
       "      <td>0.485416</td>\n",
       "      <td>-0.113209</td>\n",
       "      <td>0.193753</td>\n",
       "      <td>0.126609</td>\n",
       "      <td>-0.125333</td>\n",
       "      <td>-0.011117</td>\n",
       "      <td>-0.053230</td>\n",
       "      <td>0.063069</td>\n",
       "      <td>-0.085200</td>\n",
       "      <td>0.248008</td>\n",
       "      <td>-0.113879</td>\n",
       "      <td>0.969142</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.068446</td>\n",
       "      <td>0.239143</td>\n",
       "      <td>-0.088375</td>\n",
       "      <td>0.680371</td>\n",
       "      <td>-0.107285</td>\n",
       "      <td>-0.086793</td>\n",
       "      <td>0.050469</td>\n",
       "      <td>-0.114316</td>\n",
       "      <td>-0.122811</td>\n",
       "      <td>0.733466</td>\n",
       "      <td>-0.088834</td>\n",
       "      <td>0.273690</td>\n",
       "      <td>-0.114222</td>\n",
       "      <td>-0.116357</td>\n",
       "      <td>-0.060485</td>\n",
       "      <td>-0.080096</td>\n",
       "      <td>-0.114231</td>\n",
       "      <td>-0.073862</td>\n",
       "      <td>-0.098380</td>\n",
       "      <td>-0.112004</td>\n",
       "      <td>-0.122159</td>\n",
       "      <td>0.455277</td>\n",
       "      <td>-0.047073</td>\n",
       "      <td>-0.132610</td>\n",
       "      <td>-0.127592</td>\n",
       "      <td>-0.069297</td>\n",
       "      <td>0.386599</td>\n",
       "      <td>-0.114666</td>\n",
       "      <td>0.312522</td>\n",
       "      <td>0.101726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.098286</td>\n",
       "      <td>-0.099954</td>\n",
       "      <td>-0.073080</td>\n",
       "      <td>0.293693</td>\n",
       "      <td>0.327439</td>\n",
       "      <td>0.424508</td>\n",
       "      <td>0.117842</td>\n",
       "      <td>-0.105193</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>-0.068864</td>\n",
       "      <td>-0.113596</td>\n",
       "      <td>-0.103659</td>\n",
       "      <td>-0.095560</td>\n",
       "      <td>-0.098380</td>\n",
       "      <td>0.289770</td>\n",
       "      <td>-0.070396</td>\n",
       "      <td>0.368481</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.077985</td>\n",
       "      <td>-0.103401</td>\n",
       "      <td>-0.072822</td>\n",
       "      <td>0.125669</td>\n",
       "      <td>-0.108211</td>\n",
       "      <td>-0.097858</td>\n",
       "      <td>0.873365</td>\n",
       "      <td>-0.102820</td>\n",
       "      <td>-0.112430</td>\n",
       "      <td>-0.101254</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.091733</td>\n",
       "      <td>-0.099887</td>\n",
       "      <td>-0.093579</td>\n",
       "      <td>-0.095260</td>\n",
       "      <td>-0.098847</td>\n",
       "      <td>-0.094704</td>\n",
       "      <td>-0.104830</td>\n",
       "      <td>-0.088485</td>\n",
       "      <td>1.035236</td>\n",
       "      <td>-0.090230</td>\n",
       "      <td>-0.083635</td>\n",
       "      <td>-0.105453</td>\n",
       "      <td>-0.095568</td>\n",
       "      <td>-0.109029</td>\n",
       "      <td>-0.090898</td>\n",
       "      <td>-0.112438</td>\n",
       "      <td>-0.095990</td>\n",
       "      <td>-0.077846</td>\n",
       "      <td>-0.080340</td>\n",
       "      <td>0.385987</td>\n",
       "      <td>-0.105133</td>\n",
       "      <td>-0.090610</td>\n",
       "      <td>-0.102524</td>\n",
       "      <td>-0.080226</td>\n",
       "      <td>-0.111710</td>\n",
       "      <td>-0.022006</td>\n",
       "      <td>-0.024104</td>\n",
       "      <td>-0.102407</td>\n",
       "      <td>0.310267</td>\n",
       "      <td>-0.103706</td>\n",
       "      <td>-0.097704</td>\n",
       "      <td>-0.109111</td>\n",
       "      <td>-0.073828</td>\n",
       "      <td>-0.084230</td>\n",
       "      <td>-0.091237</td>\n",
       "      <td>-0.097104</td>\n",
       "      <td>-0.090127</td>\n",
       "      <td>-0.077063</td>\n",
       "      <td>-0.077250</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>-0.101473</td>\n",
       "      <td>-0.088599</td>\n",
       "      <td>-0.093482</td>\n",
       "      <td>-0.102430</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>-0.103217</td>\n",
       "      <td>0.879171</td>\n",
       "      <td>-0.100458</td>\n",
       "      <td>-0.044119</td>\n",
       "      <td>-0.103167</td>\n",
       "      <td>-0.109070</td>\n",
       "      <td>0.020951</td>\n",
       "      <td>-0.111249</td>\n",
       "      <td>-0.067570</td>\n",
       "      <td>-0.090442</td>\n",
       "      <td>-0.109529</td>\n",
       "      <td>-0.108943</td>\n",
       "      <td>-0.086852</td>\n",
       "      <td>-0.103018</td>\n",
       "      <td>-0.058942</td>\n",
       "      <td>-0.104964</td>\n",
       "      <td>-0.078793</td>\n",
       "      <td>-0.101075</td>\n",
       "      <td>-0.082314</td>\n",
       "      <td>-0.084838</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>-0.101992</td>\n",
       "      <td>-0.090762</td>\n",
       "      <td>0.475220</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>0.439697</td>\n",
       "      <td>0.183189</td>\n",
       "      <td>-0.110480</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>-0.084486</td>\n",
       "      <td>0.038840</td>\n",
       "      <td>-0.091652</td>\n",
       "      <td>0.352793</td>\n",
       "      <td>-0.079545</td>\n",
       "      <td>0.764845</td>\n",
       "      <td>-0.074370</td>\n",
       "      <td>-0.059496</td>\n",
       "      <td>0.128804</td>\n",
       "      <td>-0.092744</td>\n",
       "      <td>-0.111269</td>\n",
       "      <td>-0.103887</td>\n",
       "      <td>0.028374</td>\n",
       "      <td>-0.010592</td>\n",
       "      <td>-0.092135</td>\n",
       "      <td>-0.105753</td>\n",
       "      <td>0.483382</td>\n",
       "      <td>-0.094944</td>\n",
       "      <td>0.286158</td>\n",
       "      <td>-0.101178</td>\n",
       "      <td>-0.104258</td>\n",
       "      <td>-0.088571</td>\n",
       "      <td>-0.088850</td>\n",
       "      <td>-0.100128</td>\n",
       "      <td>-0.078800</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>-0.103294</td>\n",
       "      <td>-0.102136</td>\n",
       "      <td>0.284629</td>\n",
       "      <td>-0.084118</td>\n",
       "      <td>-0.102261</td>\n",
       "      <td>-0.112081</td>\n",
       "      <td>-0.079703</td>\n",
       "      <td>0.077863</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>0.055629</td>\n",
       "      <td>0.045722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48198</th>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-0.107369</td>\n",
       "      <td>-0.078000</td>\n",
       "      <td>0.223535</td>\n",
       "      <td>0.230541</td>\n",
       "      <td>0.471154</td>\n",
       "      <td>0.740946</td>\n",
       "      <td>-0.114454</td>\n",
       "      <td>-0.088883</td>\n",
       "      <td>-0.078458</td>\n",
       "      <td>-0.118445</td>\n",
       "      <td>-0.109615</td>\n",
       "      <td>-0.099668</td>\n",
       "      <td>-0.103614</td>\n",
       "      <td>0.533335</td>\n",
       "      <td>-0.093516</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.300968</td>\n",
       "      <td>0.044993</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>-0.092964</td>\n",
       "      <td>0.076197</td>\n",
       "      <td>-0.114851</td>\n",
       "      <td>-0.082166</td>\n",
       "      <td>0.715034</td>\n",
       "      <td>-0.115262</td>\n",
       "      <td>-0.116341</td>\n",
       "      <td>-0.106765</td>\n",
       "      <td>-0.118788</td>\n",
       "      <td>-0.100762</td>\n",
       "      <td>-0.107477</td>\n",
       "      <td>-0.102802</td>\n",
       "      <td>-0.103562</td>\n",
       "      <td>-0.101791</td>\n",
       "      <td>-0.103151</td>\n",
       "      <td>-0.109781</td>\n",
       "      <td>-0.090268</td>\n",
       "      <td>0.855044</td>\n",
       "      <td>-0.091621</td>\n",
       "      <td>-0.101769</td>\n",
       "      <td>-0.113936</td>\n",
       "      <td>-0.106684</td>\n",
       "      <td>-0.115684</td>\n",
       "      <td>-0.091435</td>\n",
       "      <td>-0.115706</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.082364</td>\n",
       "      <td>-0.095820</td>\n",
       "      <td>0.549308</td>\n",
       "      <td>-0.112390</td>\n",
       "      <td>-0.098033</td>\n",
       "      <td>-0.107053</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>-0.117442</td>\n",
       "      <td>-0.039271</td>\n",
       "      <td>0.055210</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>0.262131</td>\n",
       "      <td>-0.110421</td>\n",
       "      <td>-0.104865</td>\n",
       "      <td>-0.102085</td>\n",
       "      <td>-0.091681</td>\n",
       "      <td>-0.088784</td>\n",
       "      <td>-0.098899</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>-0.098828</td>\n",
       "      <td>-0.082589</td>\n",
       "      <td>-0.082951</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>-0.111164</td>\n",
       "      <td>-0.099849</td>\n",
       "      <td>-0.107379</td>\n",
       "      <td>-0.109046</td>\n",
       "      <td>0.833310</td>\n",
       "      <td>-0.108794</td>\n",
       "      <td>0.509993</td>\n",
       "      <td>-0.103918</td>\n",
       "      <td>0.117955</td>\n",
       "      <td>-0.107636</td>\n",
       "      <td>-0.108558</td>\n",
       "      <td>0.039579</td>\n",
       "      <td>-0.116049</td>\n",
       "      <td>-0.053659</td>\n",
       "      <td>-0.097434</td>\n",
       "      <td>-0.113071</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.100199</td>\n",
       "      <td>-0.107072</td>\n",
       "      <td>-0.084110</td>\n",
       "      <td>-0.101162</td>\n",
       "      <td>-0.082161</td>\n",
       "      <td>-0.105851</td>\n",
       "      <td>-0.094397</td>\n",
       "      <td>-0.088448</td>\n",
       "      <td>0.050215</td>\n",
       "      <td>-0.102431</td>\n",
       "      <td>-0.099713</td>\n",
       "      <td>0.454839</td>\n",
       "      <td>-0.104848</td>\n",
       "      <td>0.262089</td>\n",
       "      <td>0.226668</td>\n",
       "      <td>-0.117845</td>\n",
       "      <td>0.250594</td>\n",
       "      <td>-0.096402</td>\n",
       "      <td>-0.029743</td>\n",
       "      <td>-0.098394</td>\n",
       "      <td>0.337215</td>\n",
       "      <td>-0.099089</td>\n",
       "      <td>0.314727</td>\n",
       "      <td>-0.078626</td>\n",
       "      <td>-0.063640</td>\n",
       "      <td>0.073811</td>\n",
       "      <td>-0.086328</td>\n",
       "      <td>-0.085208</td>\n",
       "      <td>-0.112829</td>\n",
       "      <td>0.107655</td>\n",
       "      <td>-0.084881</td>\n",
       "      <td>-0.100405</td>\n",
       "      <td>-0.109496</td>\n",
       "      <td>0.536659</td>\n",
       "      <td>-0.103923</td>\n",
       "      <td>0.127736</td>\n",
       "      <td>-0.109245</td>\n",
       "      <td>-0.112429</td>\n",
       "      <td>-0.094747</td>\n",
       "      <td>-0.092203</td>\n",
       "      <td>-0.108622</td>\n",
       "      <td>-0.075437</td>\n",
       "      <td>-0.098587</td>\n",
       "      <td>-0.096890</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>0.421573</td>\n",
       "      <td>-0.093679</td>\n",
       "      <td>-0.110951</td>\n",
       "      <td>-0.117332</td>\n",
       "      <td>-0.088202</td>\n",
       "      <td>0.029138</td>\n",
       "      <td>-0.020140</td>\n",
       "      <td>-0.005769</td>\n",
       "      <td>0.105734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48199</th>\n",
       "      <td>-0.061787</td>\n",
       "      <td>-0.099345</td>\n",
       "      <td>-0.079985</td>\n",
       "      <td>0.165232</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.482482</td>\n",
       "      <td>0.507181</td>\n",
       "      <td>-0.118028</td>\n",
       "      <td>-0.091818</td>\n",
       "      <td>-0.062635</td>\n",
       "      <td>-0.120118</td>\n",
       "      <td>-0.105835</td>\n",
       "      <td>-0.095377</td>\n",
       "      <td>-0.101149</td>\n",
       "      <td>0.428801</td>\n",
       "      <td>-0.083683</td>\n",
       "      <td>0.413566</td>\n",
       "      <td>0.389086</td>\n",
       "      <td>0.066568</td>\n",
       "      <td>-0.108848</td>\n",
       "      <td>-0.103644</td>\n",
       "      <td>0.079706</td>\n",
       "      <td>-0.113725</td>\n",
       "      <td>-0.058829</td>\n",
       "      <td>0.653562</td>\n",
       "      <td>-0.110858</td>\n",
       "      <td>-0.116593</td>\n",
       "      <td>-0.107356</td>\n",
       "      <td>-0.121701</td>\n",
       "      <td>-0.100035</td>\n",
       "      <td>-0.102499</td>\n",
       "      <td>-0.102922</td>\n",
       "      <td>-0.102696</td>\n",
       "      <td>-0.104251</td>\n",
       "      <td>-0.099643</td>\n",
       "      <td>-0.110396</td>\n",
       "      <td>-0.093629</td>\n",
       "      <td>0.913962</td>\n",
       "      <td>-0.090032</td>\n",
       "      <td>-0.102661</td>\n",
       "      <td>-0.114622</td>\n",
       "      <td>-0.101328</td>\n",
       "      <td>-0.109310</td>\n",
       "      <td>-0.072710</td>\n",
       "      <td>-0.115118</td>\n",
       "      <td>-0.103532</td>\n",
       "      <td>-0.079873</td>\n",
       "      <td>-0.088293</td>\n",
       "      <td>0.512211</td>\n",
       "      <td>-0.112338</td>\n",
       "      <td>-0.098494</td>\n",
       "      <td>-0.110462</td>\n",
       "      <td>-0.100058</td>\n",
       "      <td>-0.117092</td>\n",
       "      <td>-0.041173</td>\n",
       "      <td>0.023582</td>\n",
       "      <td>-0.102752</td>\n",
       "      <td>0.191229</td>\n",
       "      <td>-0.111168</td>\n",
       "      <td>-0.104915</td>\n",
       "      <td>-0.110314</td>\n",
       "      <td>-0.091319</td>\n",
       "      <td>-0.087720</td>\n",
       "      <td>-0.094337</td>\n",
       "      <td>-0.113691</td>\n",
       "      <td>-0.093773</td>\n",
       "      <td>-0.084424</td>\n",
       "      <td>-0.093221</td>\n",
       "      <td>0.053221</td>\n",
       "      <td>-0.113799</td>\n",
       "      <td>-0.105581</td>\n",
       "      <td>-0.110715</td>\n",
       "      <td>-0.104822</td>\n",
       "      <td>1.113742</td>\n",
       "      <td>-0.111701</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>-0.104887</td>\n",
       "      <td>0.107861</td>\n",
       "      <td>-0.108002</td>\n",
       "      <td>-0.111247</td>\n",
       "      <td>0.043480</td>\n",
       "      <td>-0.114443</td>\n",
       "      <td>-0.066124</td>\n",
       "      <td>-0.095516</td>\n",
       "      <td>-0.111691</td>\n",
       "      <td>-0.106799</td>\n",
       "      <td>-0.097110</td>\n",
       "      <td>-0.102828</td>\n",
       "      <td>-0.063411</td>\n",
       "      <td>-0.093945</td>\n",
       "      <td>-0.084202</td>\n",
       "      <td>-0.108762</td>\n",
       "      <td>-0.101275</td>\n",
       "      <td>-0.090097</td>\n",
       "      <td>0.078232</td>\n",
       "      <td>-0.097348</td>\n",
       "      <td>-0.099030</td>\n",
       "      <td>0.389888</td>\n",
       "      <td>-0.103743</td>\n",
       "      <td>0.324171</td>\n",
       "      <td>0.168776</td>\n",
       "      <td>-0.120113</td>\n",
       "      <td>0.352780</td>\n",
       "      <td>-0.093604</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>-0.091856</td>\n",
       "      <td>0.380912</td>\n",
       "      <td>-0.089963</td>\n",
       "      <td>0.321652</td>\n",
       "      <td>-0.054262</td>\n",
       "      <td>-0.062731</td>\n",
       "      <td>0.046118</td>\n",
       "      <td>-0.082305</td>\n",
       "      <td>-0.056557</td>\n",
       "      <td>-0.103437</td>\n",
       "      <td>0.089713</td>\n",
       "      <td>-0.065964</td>\n",
       "      <td>-0.100810</td>\n",
       "      <td>-0.108442</td>\n",
       "      <td>0.396370</td>\n",
       "      <td>-0.094585</td>\n",
       "      <td>0.299330</td>\n",
       "      <td>-0.105385</td>\n",
       "      <td>-0.110923</td>\n",
       "      <td>-0.092127</td>\n",
       "      <td>-0.094027</td>\n",
       "      <td>-0.107301</td>\n",
       "      <td>-0.077611</td>\n",
       "      <td>-0.098453</td>\n",
       "      <td>-0.099868</td>\n",
       "      <td>-0.112403</td>\n",
       "      <td>0.334659</td>\n",
       "      <td>-0.088464</td>\n",
       "      <td>-0.111661</td>\n",
       "      <td>-0.116378</td>\n",
       "      <td>-0.087172</td>\n",
       "      <td>0.112799</td>\n",
       "      <td>-0.044621</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.092049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48200</th>\n",
       "      <td>-0.085076</td>\n",
       "      <td>-0.019765</td>\n",
       "      <td>-0.040139</td>\n",
       "      <td>0.082385</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.263367</td>\n",
       "      <td>-0.078611</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>-0.063911</td>\n",
       "      <td>-0.069864</td>\n",
       "      <td>-0.083776</td>\n",
       "      <td>-0.077414</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>-0.075839</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>-0.073414</td>\n",
       "      <td>0.257971</td>\n",
       "      <td>-0.079189</td>\n",
       "      <td>-0.061539</td>\n",
       "      <td>-0.076336</td>\n",
       "      <td>-0.070014</td>\n",
       "      <td>0.390727</td>\n",
       "      <td>-0.065538</td>\n",
       "      <td>-0.059764</td>\n",
       "      <td>-0.016561</td>\n",
       "      <td>-0.076601</td>\n",
       "      <td>-0.081644</td>\n",
       "      <td>-0.079352</td>\n",
       "      <td>-0.080485</td>\n",
       "      <td>-0.075533</td>\n",
       "      <td>-0.071951</td>\n",
       "      <td>-0.073284</td>\n",
       "      <td>-0.072428</td>\n",
       "      <td>-0.076604</td>\n",
       "      <td>-0.069523</td>\n",
       "      <td>-0.081019</td>\n",
       "      <td>-0.064263</td>\n",
       "      <td>0.746180</td>\n",
       "      <td>-0.056343</td>\n",
       "      <td>-0.070689</td>\n",
       "      <td>-0.077235</td>\n",
       "      <td>-0.077561</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>0.140361</td>\n",
       "      <td>-0.085641</td>\n",
       "      <td>-0.070548</td>\n",
       "      <td>-0.064903</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>-0.101629</td>\n",
       "      <td>-0.078929</td>\n",
       "      <td>-0.072299</td>\n",
       "      <td>-0.074348</td>\n",
       "      <td>-0.065689</td>\n",
       "      <td>-0.082947</td>\n",
       "      <td>-0.043394</td>\n",
       "      <td>-0.024674</td>\n",
       "      <td>-0.071774</td>\n",
       "      <td>-0.056329</td>\n",
       "      <td>-0.076296</td>\n",
       "      <td>-0.073641</td>\n",
       "      <td>-0.080424</td>\n",
       "      <td>-0.048682</td>\n",
       "      <td>-0.066995</td>\n",
       "      <td>-0.070810</td>\n",
       "      <td>-0.077974</td>\n",
       "      <td>-0.074897</td>\n",
       "      <td>-0.060521</td>\n",
       "      <td>-0.063480</td>\n",
       "      <td>0.051425</td>\n",
       "      <td>-0.083099</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.081671</td>\n",
       "      <td>-0.074626</td>\n",
       "      <td>-0.011034</td>\n",
       "      <td>-0.078993</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>-0.069220</td>\n",
       "      <td>-0.076706</td>\n",
       "      <td>-0.072964</td>\n",
       "      <td>0.145734</td>\n",
       "      <td>-0.064544</td>\n",
       "      <td>0.140592</td>\n",
       "      <td>-0.059370</td>\n",
       "      <td>-0.063342</td>\n",
       "      <td>-0.079359</td>\n",
       "      <td>-0.081457</td>\n",
       "      <td>-0.067831</td>\n",
       "      <td>-0.075096</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>1.179554</td>\n",
       "      <td>-0.063517</td>\n",
       "      <td>-0.077332</td>\n",
       "      <td>-0.067052</td>\n",
       "      <td>-0.047125</td>\n",
       "      <td>-0.027744</td>\n",
       "      <td>-0.052680</td>\n",
       "      <td>0.057587</td>\n",
       "      <td>0.327698</td>\n",
       "      <td>-0.072898</td>\n",
       "      <td>0.466532</td>\n",
       "      <td>0.093638</td>\n",
       "      <td>-0.081686</td>\n",
       "      <td>-0.075682</td>\n",
       "      <td>-0.068885</td>\n",
       "      <td>-0.044305</td>\n",
       "      <td>-0.070817</td>\n",
       "      <td>0.159998</td>\n",
       "      <td>-0.078361</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.061928</td>\n",
       "      <td>-0.075957</td>\n",
       "      <td>0.106712</td>\n",
       "      <td>-0.077459</td>\n",
       "      <td>1.399233</td>\n",
       "      <td>-0.078626</td>\n",
       "      <td>-0.072083</td>\n",
       "      <td>-0.030713</td>\n",
       "      <td>-0.080510</td>\n",
       "      <td>-0.076025</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>-0.066634</td>\n",
       "      <td>0.977552</td>\n",
       "      <td>-0.074948</td>\n",
       "      <td>-0.076043</td>\n",
       "      <td>-0.065239</td>\n",
       "      <td>-0.067072</td>\n",
       "      <td>-0.078083</td>\n",
       "      <td>-0.072537</td>\n",
       "      <td>-0.073559</td>\n",
       "      <td>-0.016842</td>\n",
       "      <td>-0.084288</td>\n",
       "      <td>0.075517</td>\n",
       "      <td>-0.068157</td>\n",
       "      <td>-0.082042</td>\n",
       "      <td>-0.075769</td>\n",
       "      <td>-0.035932</td>\n",
       "      <td>0.073994</td>\n",
       "      <td>-0.071730</td>\n",
       "      <td>-0.055548</td>\n",
       "      <td>0.074910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48201</th>\n",
       "      <td>-0.129128</td>\n",
       "      <td>-0.097133</td>\n",
       "      <td>-0.064378</td>\n",
       "      <td>0.162504</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>0.240771</td>\n",
       "      <td>-0.116835</td>\n",
       "      <td>-0.087210</td>\n",
       "      <td>-0.052549</td>\n",
       "      <td>-0.122759</td>\n",
       "      <td>-0.147196</td>\n",
       "      <td>-0.094668</td>\n",
       "      <td>-0.111958</td>\n",
       "      <td>-0.072857</td>\n",
       "      <td>0.228428</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>0.152343</td>\n",
       "      <td>-0.079240</td>\n",
       "      <td>-0.124319</td>\n",
       "      <td>-0.094852</td>\n",
       "      <td>-0.056010</td>\n",
       "      <td>0.237674</td>\n",
       "      <td>-0.138501</td>\n",
       "      <td>-0.093899</td>\n",
       "      <td>0.322706</td>\n",
       "      <td>-0.115159</td>\n",
       "      <td>-0.138109</td>\n",
       "      <td>-0.150201</td>\n",
       "      <td>-0.104187</td>\n",
       "      <td>-0.104026</td>\n",
       "      <td>-0.109871</td>\n",
       "      <td>-0.079260</td>\n",
       "      <td>-0.112356</td>\n",
       "      <td>-0.112003</td>\n",
       "      <td>-0.110327</td>\n",
       "      <td>-0.127146</td>\n",
       "      <td>-0.017746</td>\n",
       "      <td>0.389237</td>\n",
       "      <td>-0.046356</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>-0.112604</td>\n",
       "      <td>-0.068824</td>\n",
       "      <td>-0.104885</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>-0.152767</td>\n",
       "      <td>-0.113220</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>-0.138697</td>\n",
       "      <td>-0.079695</td>\n",
       "      <td>-0.120572</td>\n",
       "      <td>-0.048413</td>\n",
       "      <td>-0.101269</td>\n",
       "      <td>-0.118785</td>\n",
       "      <td>-0.136030</td>\n",
       "      <td>0.026797</td>\n",
       "      <td>-0.033499</td>\n",
       "      <td>-0.096910</td>\n",
       "      <td>0.092198</td>\n",
       "      <td>-0.118307</td>\n",
       "      <td>-0.105043</td>\n",
       "      <td>-0.137010</td>\n",
       "      <td>-0.069529</td>\n",
       "      <td>-0.039909</td>\n",
       "      <td>-0.074379</td>\n",
       "      <td>-0.137791</td>\n",
       "      <td>-0.032580</td>\n",
       "      <td>0.045669</td>\n",
       "      <td>-0.046871</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>-0.141396</td>\n",
       "      <td>-0.114719</td>\n",
       "      <td>-0.128153</td>\n",
       "      <td>-0.103959</td>\n",
       "      <td>0.298170</td>\n",
       "      <td>-0.143355</td>\n",
       "      <td>0.436532</td>\n",
       "      <td>-0.094557</td>\n",
       "      <td>0.259603</td>\n",
       "      <td>-0.113748</td>\n",
       "      <td>-0.127084</td>\n",
       "      <td>-0.126154</td>\n",
       "      <td>-0.177546</td>\n",
       "      <td>-0.090571</td>\n",
       "      <td>-0.067359</td>\n",
       "      <td>-0.145295</td>\n",
       "      <td>-0.140265</td>\n",
       "      <td>-0.075745</td>\n",
       "      <td>-0.120720</td>\n",
       "      <td>0.103899</td>\n",
       "      <td>0.651415</td>\n",
       "      <td>-0.007870</td>\n",
       "      <td>-0.121442</td>\n",
       "      <td>-0.096351</td>\n",
       "      <td>-0.026074</td>\n",
       "      <td>-0.095603</td>\n",
       "      <td>-0.086581</td>\n",
       "      <td>-0.050615</td>\n",
       "      <td>0.532529</td>\n",
       "      <td>-0.112513</td>\n",
       "      <td>0.111921</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>-0.130919</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>-0.034589</td>\n",
       "      <td>0.211631</td>\n",
       "      <td>-0.080331</td>\n",
       "      <td>0.338320</td>\n",
       "      <td>-0.113341</td>\n",
       "      <td>1.001636</td>\n",
       "      <td>0.099866</td>\n",
       "      <td>0.150150</td>\n",
       "      <td>0.050946</td>\n",
       "      <td>-0.063190</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>-0.104874</td>\n",
       "      <td>-0.090285</td>\n",
       "      <td>0.043555</td>\n",
       "      <td>-0.120122</td>\n",
       "      <td>-0.128133</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>-0.086178</td>\n",
       "      <td>0.036741</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-0.115373</td>\n",
       "      <td>-0.039178</td>\n",
       "      <td>-0.054202</td>\n",
       "      <td>-0.111994</td>\n",
       "      <td>-0.058026</td>\n",
       "      <td>-0.090350</td>\n",
       "      <td>-0.102356</td>\n",
       "      <td>-0.124435</td>\n",
       "      <td>0.485146</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>-0.137868</td>\n",
       "      <td>-0.138146</td>\n",
       "      <td>-0.082405</td>\n",
       "      <td>0.565141</td>\n",
       "      <td>-0.111353</td>\n",
       "      <td>0.437580</td>\n",
       "      <td>0.196572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48202</th>\n",
       "      <td>-0.128947</td>\n",
       "      <td>-0.098698</td>\n",
       "      <td>-0.057946</td>\n",
       "      <td>0.218069</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.350670</td>\n",
       "      <td>-0.102106</td>\n",
       "      <td>-0.103211</td>\n",
       "      <td>-0.043552</td>\n",
       "      <td>-0.122771</td>\n",
       "      <td>-0.151379</td>\n",
       "      <td>-0.090858</td>\n",
       "      <td>-0.109797</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>0.254119</td>\n",
       "      <td>-0.140788</td>\n",
       "      <td>0.047334</td>\n",
       "      <td>-0.110243</td>\n",
       "      <td>-0.089642</td>\n",
       "      <td>-0.105401</td>\n",
       "      <td>0.020027</td>\n",
       "      <td>0.257454</td>\n",
       "      <td>-0.131807</td>\n",
       "      <td>-0.100073</td>\n",
       "      <td>0.455913</td>\n",
       "      <td>-0.115058</td>\n",
       "      <td>-0.134402</td>\n",
       "      <td>-0.150518</td>\n",
       "      <td>-0.096555</td>\n",
       "      <td>-0.100991</td>\n",
       "      <td>-0.096047</td>\n",
       "      <td>-0.072772</td>\n",
       "      <td>-0.113729</td>\n",
       "      <td>-0.108110</td>\n",
       "      <td>-0.108629</td>\n",
       "      <td>-0.129048</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.105450</td>\n",
       "      <td>-0.022735</td>\n",
       "      <td>-0.081542</td>\n",
       "      <td>-0.111657</td>\n",
       "      <td>-0.058725</td>\n",
       "      <td>-0.092041</td>\n",
       "      <td>0.102044</td>\n",
       "      <td>-0.166412</td>\n",
       "      <td>-0.114152</td>\n",
       "      <td>0.049650</td>\n",
       "      <td>-0.150111</td>\n",
       "      <td>-0.080340</td>\n",
       "      <td>-0.112300</td>\n",
       "      <td>-0.051613</td>\n",
       "      <td>-0.097090</td>\n",
       "      <td>-0.123863</td>\n",
       "      <td>-0.134393</td>\n",
       "      <td>0.064611</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>-0.089098</td>\n",
       "      <td>-0.029789</td>\n",
       "      <td>-0.117498</td>\n",
       "      <td>-0.093825</td>\n",
       "      <td>-0.127566</td>\n",
       "      <td>-0.069902</td>\n",
       "      <td>-0.030781</td>\n",
       "      <td>-0.073822</td>\n",
       "      <td>-0.131091</td>\n",
       "      <td>-0.028080</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>-0.031719</td>\n",
       "      <td>0.272981</td>\n",
       "      <td>-0.151164</td>\n",
       "      <td>-0.115490</td>\n",
       "      <td>-0.131746</td>\n",
       "      <td>-0.099117</td>\n",
       "      <td>0.151749</td>\n",
       "      <td>-0.147787</td>\n",
       "      <td>0.347624</td>\n",
       "      <td>-0.081263</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>-0.113392</td>\n",
       "      <td>-0.127923</td>\n",
       "      <td>-0.126183</td>\n",
       "      <td>-0.172373</td>\n",
       "      <td>-0.084244</td>\n",
       "      <td>-0.058189</td>\n",
       "      <td>-0.144182</td>\n",
       "      <td>-0.143156</td>\n",
       "      <td>-0.071269</td>\n",
       "      <td>-0.123507</td>\n",
       "      <td>0.130226</td>\n",
       "      <td>0.345327</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>-0.115061</td>\n",
       "      <td>-0.089600</td>\n",
       "      <td>-0.019476</td>\n",
       "      <td>-0.103571</td>\n",
       "      <td>-0.105724</td>\n",
       "      <td>-0.027330</td>\n",
       "      <td>0.560776</td>\n",
       "      <td>-0.111846</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>0.053995</td>\n",
       "      <td>-0.130140</td>\n",
       "      <td>-0.031976</td>\n",
       "      <td>-0.029330</td>\n",
       "      <td>0.118122</td>\n",
       "      <td>-0.063430</td>\n",
       "      <td>0.268391</td>\n",
       "      <td>-0.110483</td>\n",
       "      <td>0.842368</td>\n",
       "      <td>0.139439</td>\n",
       "      <td>0.182897</td>\n",
       "      <td>0.143093</td>\n",
       "      <td>-0.054636</td>\n",
       "      <td>1.099660</td>\n",
       "      <td>-0.102023</td>\n",
       "      <td>-0.096397</td>\n",
       "      <td>0.271296</td>\n",
       "      <td>-0.111770</td>\n",
       "      <td>-0.119681</td>\n",
       "      <td>0.855621</td>\n",
       "      <td>-0.078897</td>\n",
       "      <td>0.037913</td>\n",
       "      <td>-0.109265</td>\n",
       "      <td>-0.112099</td>\n",
       "      <td>-0.038798</td>\n",
       "      <td>-0.056705</td>\n",
       "      <td>-0.111807</td>\n",
       "      <td>-0.042499</td>\n",
       "      <td>-0.086195</td>\n",
       "      <td>-0.116125</td>\n",
       "      <td>-0.126190</td>\n",
       "      <td>0.449037</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>-0.144969</td>\n",
       "      <td>-0.139429</td>\n",
       "      <td>-0.067139</td>\n",
       "      <td>0.731363</td>\n",
       "      <td>-0.114612</td>\n",
       "      <td>0.458202</td>\n",
       "      <td>0.194715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48203 rows  140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.076877 -0.107536 -0.081412  0.235340  0.212383  0.508778  0.714871   \n",
       "1     -0.070980 -0.105483 -0.078728  0.238921  0.227432  0.494300  0.868887   \n",
       "2     -0.089150 -0.101540 -0.076435  0.338878  0.290691  0.323657  1.014777   \n",
       "3     -0.128470 -0.113081 -0.063264  0.226137  0.264680  0.215720 -0.138097   \n",
       "4     -0.098286 -0.099954 -0.073080  0.293693  0.327439  0.424508  0.117842   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.016057 -0.107369 -0.078000  0.223535  0.230541  0.471154  0.740946   \n",
       "48199 -0.061787 -0.099345 -0.079985  0.165232  0.196600  0.482482  0.507181   \n",
       "48200 -0.085076 -0.019765 -0.040139  0.082385  0.178649  0.263367 -0.078611   \n",
       "48201 -0.129128 -0.097133 -0.064378  0.162504  0.062285  0.240771 -0.116835   \n",
       "48202 -0.128947 -0.098698 -0.057946  0.218069  0.022791  0.350670 -0.102106   \n",
       "\n",
       "            7         8         9         10        11        12        13   \\\n",
       "0     -0.111080 -0.094659 -0.073315 -0.116281 -0.108862 -0.097573 -0.104481   \n",
       "1     -0.106807 -0.090079 -0.077481 -0.116207 -0.105719 -0.089185 -0.101150   \n",
       "2     -0.105268 -0.083472 -0.092418 -0.112782 -0.102729 -0.092453 -0.097368   \n",
       "3     -0.083314 -0.076588 -0.114464 -0.140753 -0.099502 -0.109232 -0.089628   \n",
       "4     -0.105193 -0.083176 -0.068864 -0.113596 -0.103659 -0.095560 -0.098380   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.114454 -0.088883 -0.078458 -0.118445 -0.109615 -0.099668 -0.103614   \n",
       "48199 -0.118028 -0.091818 -0.062635 -0.120118 -0.105835 -0.095377 -0.101149   \n",
       "48200  0.071203 -0.063911 -0.069864 -0.083776 -0.077414 -0.075413 -0.075839   \n",
       "48201 -0.087210 -0.052549 -0.122759 -0.147196 -0.094668 -0.111958 -0.072857   \n",
       "48202 -0.103211 -0.043552 -0.122771 -0.151379 -0.090858 -0.109797 -0.072558   \n",
       "\n",
       "            14        15        16        17        18        19        20   \\\n",
       "0      0.483716 -0.079324  0.426531  0.302832  0.053903 -0.107653 -0.082257   \n",
       "1      0.476003 -0.084783  0.395964  0.322830  0.022176 -0.101084 -0.087909   \n",
       "2      0.790603 -0.063658  0.255258  0.259364 -0.007930 -0.099144 -0.079277   \n",
       "3      0.474046 -0.116054  0.243485 -0.126989 -0.007528 -0.024485 -0.085576   \n",
       "4      0.289770 -0.070396  0.368481  0.002567  0.077985 -0.103401 -0.072822   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198  0.533335 -0.093516  0.431017  0.300968  0.044993 -0.107427 -0.092964   \n",
       "48199  0.428801 -0.083683  0.413566  0.389086  0.066568 -0.108848 -0.103644   \n",
       "48200 -0.033571 -0.073414  0.257971 -0.079189 -0.061539 -0.076336 -0.070014   \n",
       "48201  0.228428 -0.128448  0.152343 -0.079240 -0.124319 -0.094852 -0.056010   \n",
       "48202  0.254119 -0.140788  0.047334 -0.110243 -0.089642 -0.105401  0.020027   \n",
       "\n",
       "            21        22        23        24        25        26        27   \\\n",
       "0      0.090119 -0.112168 -0.095676  0.687361 -0.113379 -0.115403 -0.108167   \n",
       "1      0.063508 -0.109683 -0.085107  0.715892 -0.106425 -0.111527 -0.107168   \n",
       "2      0.090837 -0.107481 -0.086070  0.681157 -0.103144 -0.109113 -0.100578   \n",
       "3      0.149265 -0.118302 -0.085031  0.411208 -0.119124 -0.130412 -0.142772   \n",
       "4      0.125669 -0.108211 -0.097858  0.873365 -0.102820 -0.112430 -0.101254   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198  0.076197 -0.114851 -0.082166  0.715034 -0.115262 -0.116341 -0.106765   \n",
       "48199  0.079706 -0.113725 -0.058829  0.653562 -0.110858 -0.116593 -0.107356   \n",
       "48200  0.390727 -0.065538 -0.059764 -0.016561 -0.076601 -0.081644 -0.079352   \n",
       "48201  0.237674 -0.138501 -0.093899  0.322706 -0.115159 -0.138109 -0.150201   \n",
       "48202  0.257454 -0.131807 -0.100073  0.455913 -0.115058 -0.134402 -0.150518   \n",
       "\n",
       "            28        29        30        31        32        33        34   \\\n",
       "0     -0.117696 -0.108534 -0.111692 -0.104854 -0.105364 -0.105601 -0.103499   \n",
       "1     -0.115465 -0.103104 -0.106192 -0.099530 -0.101643 -0.100825 -0.101097   \n",
       "2     -0.122811 -0.096156 -0.102624 -0.092398 -0.097464 -0.097798 -0.095754   \n",
       "3     -0.121572 -0.107517 -0.112074 -0.087340 -0.113672 -0.113528 -0.107757   \n",
       "4     -0.118073 -0.091733 -0.099887 -0.093579 -0.095260 -0.098847 -0.094704   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.118788 -0.100762 -0.107477 -0.102802 -0.103562 -0.101791 -0.103151   \n",
       "48199 -0.121701 -0.100035 -0.102499 -0.102922 -0.102696 -0.104251 -0.099643   \n",
       "48200 -0.080485 -0.075533 -0.071951 -0.073284 -0.072428 -0.076604 -0.069523   \n",
       "48201 -0.104187 -0.104026 -0.109871 -0.079260 -0.112356 -0.112003 -0.110327   \n",
       "48202 -0.096555 -0.100991 -0.096047 -0.072772 -0.113729 -0.108110 -0.108629   \n",
       "\n",
       "            35        36        37        38        39        40        41   \\\n",
       "0     -0.111801 -0.088663  0.869081 -0.095526 -0.093547 -0.110812 -0.105767   \n",
       "1     -0.109965 -0.090401  0.831216 -0.089985 -0.093495 -0.107225 -0.103154   \n",
       "2     -0.104699 -0.085765  0.723438 -0.087787 -0.088923 -0.104203 -0.101058   \n",
       "3     -0.124679 -0.049125  0.723409 -0.068823 -0.082963 -0.110676 -0.088543   \n",
       "4     -0.104830 -0.088485  1.035236 -0.090230 -0.083635 -0.105453 -0.095568   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.109781 -0.090268  0.855044 -0.091621 -0.101769 -0.113936 -0.106684   \n",
       "48199 -0.110396 -0.093629  0.913962 -0.090032 -0.102661 -0.114622 -0.101328   \n",
       "48200 -0.081019 -0.064263  0.746180 -0.056343 -0.070689 -0.077235 -0.077561   \n",
       "48201 -0.127146 -0.017746  0.389237 -0.046356 -0.084448 -0.112604 -0.068824   \n",
       "48202 -0.129048 -0.004362  0.105450 -0.022735 -0.081542 -0.111657 -0.058725   \n",
       "\n",
       "            42        43        44        45        46        47        48   \\\n",
       "0     -0.112340 -0.092821 -0.116772 -0.105450 -0.081068 -0.082683  0.610800   \n",
       "1     -0.108755 -0.103866 -0.113471 -0.102514 -0.077068 -0.083564  0.589613   \n",
       "2     -0.102143 -0.045539 -0.107397 -0.097045 -0.074226 -0.085142  0.200093   \n",
       "3     -0.105773  0.603149 -0.146289 -0.110784 -0.024087 -0.115214 -0.053530   \n",
       "4     -0.109029 -0.090898 -0.112438 -0.095990 -0.077846 -0.080340  0.385987   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.115684 -0.091435 -0.115706 -0.101562 -0.082364 -0.095820  0.549308   \n",
       "48199 -0.109310 -0.072710 -0.115118 -0.103532 -0.079873 -0.088293  0.512211   \n",
       "48200 -0.075413  0.140361 -0.085641 -0.070548 -0.064903  0.045767 -0.101629   \n",
       "48201 -0.104885  0.212656 -0.152767 -0.113220  0.018889 -0.138697 -0.079695   \n",
       "48202 -0.092041  0.102044 -0.166412 -0.114152  0.049650 -0.150111 -0.080340   \n",
       "\n",
       "            49        50        51        52        53        54        55   \\\n",
       "0     -0.109826 -0.104239 -0.105984 -0.094111 -0.115045 -0.020432  0.026682   \n",
       "1     -0.106611 -0.099559 -0.103316 -0.106078 -0.110636 -0.025642  0.042071   \n",
       "2     -0.104822 -0.092768 -0.099995 -0.080043 -0.107574 -0.019395  0.032191   \n",
       "3     -0.118277 -0.074823 -0.102637 -0.066636 -0.129014  0.016024 -0.050958   \n",
       "4     -0.105133 -0.090610 -0.102524 -0.080226 -0.111710 -0.022006 -0.024104   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.112390 -0.098033 -0.107053 -0.089752 -0.117442 -0.039271  0.055210   \n",
       "48199 -0.112338 -0.098494 -0.110462 -0.100058 -0.117092 -0.041173  0.023582   \n",
       "48200 -0.078929 -0.072299 -0.074348 -0.065689 -0.082947 -0.043394 -0.024674   \n",
       "48201 -0.120572 -0.048413 -0.101269 -0.118785 -0.136030  0.026797 -0.033499   \n",
       "48202 -0.112300 -0.051613 -0.097090 -0.123863 -0.134393  0.064611 -0.004215   \n",
       "\n",
       "            56        57        58        59        60        61        62   \\\n",
       "0     -0.105935  0.272084 -0.105307 -0.106435 -0.111940 -0.097159 -0.089999   \n",
       "1     -0.102003  0.180827 -0.103470 -0.103296 -0.104360 -0.095484 -0.088204   \n",
       "2     -0.101605  0.177870 -0.100262 -0.098974 -0.051951 -0.086125 -0.083883   \n",
       "3     -0.101564  0.207655 -0.116863 -0.108798 -0.131065 -0.084988 -0.066779   \n",
       "4     -0.102407  0.310267 -0.103706 -0.097704 -0.109111 -0.073828 -0.084230   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.108676  0.262131 -0.110421 -0.104865 -0.102085 -0.091681 -0.088784   \n",
       "48199 -0.102752  0.191229 -0.111168 -0.104915 -0.110314 -0.091319 -0.087720   \n",
       "48200 -0.071774 -0.056329 -0.076296 -0.073641 -0.080424 -0.048682 -0.066995   \n",
       "48201 -0.096910  0.092198 -0.118307 -0.105043 -0.137010 -0.069529 -0.039909   \n",
       "48202 -0.089098 -0.029789 -0.117498 -0.093825 -0.127566 -0.069902 -0.030781   \n",
       "\n",
       "            63        64        65        66        67        68        69   \\\n",
       "0     -0.103777 -0.112325 -0.095288 -0.079516 -0.093184  0.072477 -0.109951   \n",
       "1     -0.098695 -0.108114 -0.093327 -0.078601 -0.091153  0.080918 -0.108536   \n",
       "2     -0.091763 -0.102663 -0.089455 -0.072468 -0.088659  0.086627 -0.104616   \n",
       "3     -0.093403 -0.129859 -0.053791  0.004140 -0.051470  0.141465 -0.130556   \n",
       "4     -0.091237 -0.097104 -0.090127 -0.077063 -0.077250  0.059001 -0.101473   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.098899 -0.115326 -0.098828 -0.082589 -0.082951  0.076279 -0.111164   \n",
       "48199 -0.094337 -0.113691 -0.093773 -0.084424 -0.093221  0.053221 -0.113799   \n",
       "48200 -0.070810 -0.077974 -0.074897 -0.060521 -0.063480  0.051425 -0.083099   \n",
       "48201 -0.074379 -0.137791 -0.032580  0.045669 -0.046871  0.210007 -0.141396   \n",
       "48202 -0.073822 -0.131091 -0.028080  0.093197 -0.031719  0.272981 -0.151164   \n",
       "\n",
       "            70        71        72        73        74        75        76   \\\n",
       "0     -0.110628 -0.118938 -0.107578  0.769669 -0.110701  0.448382 -0.105106   \n",
       "1     -0.103518 -0.112984 -0.104845  0.734899 -0.107256  0.403173 -0.101109   \n",
       "2     -0.094370 -0.107391 -0.101939  0.314863 -0.100397  0.283052 -0.099730   \n",
       "3     -0.115734 -0.124357 -0.109641  0.041874 -0.134306  0.284571 -0.099015   \n",
       "4     -0.088599 -0.093482 -0.102430  0.404472 -0.103217  0.879171 -0.100458   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.099849 -0.107379 -0.109046  0.833310 -0.108794  0.509993 -0.103918   \n",
       "48199 -0.105581 -0.110715 -0.104822  1.113742 -0.111701  0.482917 -0.104887   \n",
       "48200 -0.077015 -0.081671 -0.074626 -0.011034 -0.078993  0.039805 -0.069220   \n",
       "48201 -0.114719 -0.128153 -0.103959  0.298170 -0.143355  0.436532 -0.094557   \n",
       "48202 -0.115490 -0.131746 -0.099117  0.151749 -0.147787  0.347624 -0.081263   \n",
       "\n",
       "            77        78        79        80        81        82        83   \\\n",
       "0      0.076690 -0.108419 -0.109099  0.042020 -0.113356 -0.060216 -0.099415   \n",
       "1      0.039439 -0.103329 -0.103254  0.018683 -0.110621 -0.056592 -0.095719   \n",
       "2      0.002265 -0.099223 -0.102801 -0.019462 -0.103728 -0.036343 -0.091792   \n",
       "3     -0.003217 -0.113088 -0.116721 -0.081502 -0.142544 -0.060783 -0.077973   \n",
       "4     -0.044119 -0.103167 -0.109070  0.020951 -0.111249 -0.067570 -0.090442   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198  0.117955 -0.107636 -0.108558  0.039579 -0.116049 -0.053659 -0.097434   \n",
       "48199  0.107861 -0.108002 -0.111247  0.043480 -0.114443 -0.066124 -0.095516   \n",
       "48200 -0.076706 -0.072964  0.145734 -0.064544  0.140592 -0.059370 -0.063342   \n",
       "48201  0.259603 -0.113748 -0.127084 -0.126154 -0.177546 -0.090571 -0.067359   \n",
       "48202  0.089772 -0.113392 -0.127923 -0.126183 -0.172373 -0.084244 -0.058189   \n",
       "\n",
       "            84        85        86        87        88        89        90   \\\n",
       "0     -0.112301 -0.113646 -0.097630 -0.107329 -0.081830 -0.101124 -0.079099   \n",
       "1     -0.110038 -0.108561 -0.095397 -0.102081 -0.078001 -0.104646 -0.079995   \n",
       "2     -0.109485 -0.104571 -0.091584 -0.099345 -0.069711 -0.100305 -0.074391   \n",
       "3     -0.138675 -0.134785 -0.087301 -0.119694  0.048126  0.725502 -0.036445   \n",
       "4     -0.109529 -0.108943 -0.086852 -0.103018 -0.058942 -0.104964 -0.078793   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.113071 -0.109205 -0.100199 -0.107072 -0.084110 -0.101162 -0.082161   \n",
       "48199 -0.111691 -0.106799 -0.097110 -0.102828 -0.063411 -0.093945 -0.084202   \n",
       "48200 -0.079359 -0.081457 -0.067831 -0.075096  0.043017  1.179554 -0.063517   \n",
       "48201 -0.145295 -0.140265 -0.075745 -0.120720  0.103899  0.651415 -0.007870   \n",
       "48202 -0.144182 -0.143156 -0.071269 -0.123507  0.130226  0.345327  0.017902   \n",
       "\n",
       "            91        92        93        94        95        96        97   \\\n",
       "0     -0.110585 -0.103025 -0.086625  0.053080 -0.096740 -0.099395  0.420336   \n",
       "1     -0.105553 -0.101321 -0.083420  0.043079 -0.092405 -0.097549  0.416196   \n",
       "2     -0.102350 -0.090838 -0.080520  0.039465 -0.090182 -0.092018  0.526895   \n",
       "3     -0.120802 -0.100612 -0.033217 -0.031502 -0.067413 -0.078095  0.485416   \n",
       "4     -0.101075 -0.082314 -0.084838  0.104553 -0.101992 -0.090762  0.475220   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.105851 -0.094397 -0.088448  0.050215 -0.102431 -0.099713  0.454839   \n",
       "48199 -0.108762 -0.101275 -0.090097  0.078232 -0.097348 -0.099030  0.389888   \n",
       "48200 -0.077332 -0.067052 -0.047125 -0.027744 -0.052680  0.057587  0.327698   \n",
       "48201 -0.121442 -0.096351 -0.026074 -0.095603 -0.086581 -0.050615  0.532529   \n",
       "48202 -0.115061 -0.089600 -0.019476 -0.103571 -0.105724 -0.027330  0.560776   \n",
       "\n",
       "            98        99        100       101       102       103       104  \\\n",
       "0     -0.103349  0.363882  0.214918 -0.115309  0.262475 -0.095930  0.022537   \n",
       "1     -0.102180  0.356702  0.219733 -0.110612  0.246460 -0.093005 -0.011302   \n",
       "2     -0.098591  0.248171  0.198832 -0.108625  0.113670 -0.087849  0.206987   \n",
       "3     -0.113209  0.193753  0.126609 -0.125333 -0.011117 -0.053230  0.063069   \n",
       "4     -0.095363  0.439697  0.183189 -0.110480  0.095128 -0.084486  0.038840   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.104848  0.262089  0.226668 -0.117845  0.250594 -0.096402 -0.029743   \n",
       "48199 -0.103743  0.324171  0.168776 -0.120113  0.352780 -0.093604 -0.028148   \n",
       "48200 -0.072898  0.466532  0.093638 -0.081686 -0.075682 -0.068885 -0.044305   \n",
       "48201 -0.112513  0.111921  0.001366 -0.130919  0.052534 -0.034589  0.211631   \n",
       "48202 -0.111846  0.056695  0.053995 -0.130140 -0.031976 -0.029330  0.118122   \n",
       "\n",
       "            105       106       107       108       109       110       111  \\\n",
       "0     -0.092371  0.312263 -0.070511  0.484739 -0.072997 -0.068368  0.107584   \n",
       "1     -0.092891  0.270515 -0.075828  0.516748 -0.078225 -0.073941  0.042847   \n",
       "2     -0.087614  0.187265 -0.094517  0.585659 -0.078287 -0.062720 -0.034883   \n",
       "3     -0.085200  0.248008 -0.113879  0.969142  0.020769  0.068446  0.239143   \n",
       "4     -0.091652  0.352793 -0.079545  0.764845 -0.074370 -0.059496  0.128804   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.098394  0.337215 -0.099089  0.314727 -0.078626 -0.063640  0.073811   \n",
       "48199 -0.091856  0.380912 -0.089963  0.321652 -0.054262 -0.062731  0.046118   \n",
       "48200 -0.070817  0.159998 -0.078361  0.000312 -0.061928 -0.075957  0.106712   \n",
       "48201 -0.080331  0.338320 -0.113341  1.001636  0.099866  0.150150  0.050946   \n",
       "48202 -0.063430  0.268391 -0.110483  0.842368  0.139439  0.182897  0.143093   \n",
       "\n",
       "            112       113       114       115       116       117       118  \\\n",
       "0     -0.087482 -0.107399 -0.112273  0.073439 -0.038099 -0.095309 -0.102715   \n",
       "1     -0.080481 -0.097804 -0.106804  0.024413 -0.055215 -0.090355 -0.100601   \n",
       "2     -0.078428 -0.089400 -0.101648 -0.047284 -0.077511 -0.092484 -0.060043   \n",
       "3     -0.088375  0.680371 -0.107285 -0.086793  0.050469 -0.114316 -0.122811   \n",
       "4     -0.092744 -0.111269 -0.103887  0.028374 -0.010592 -0.092135 -0.105753   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.086328 -0.085208 -0.112829  0.107655 -0.084881 -0.100405 -0.109496   \n",
       "48199 -0.082305 -0.056557 -0.103437  0.089713 -0.065964 -0.100810 -0.108442   \n",
       "48200 -0.077459  1.399233 -0.078626 -0.072083 -0.030713 -0.080510 -0.076025   \n",
       "48201 -0.063190  0.886158 -0.104874 -0.090285  0.043555 -0.120122 -0.128133   \n",
       "48202 -0.054636  1.099660 -0.102023 -0.096397  0.271296 -0.111770 -0.119681   \n",
       "\n",
       "            119       120       121       122       123       124       125  \\\n",
       "0      0.572418 -0.099416  0.070323 -0.106917 -0.111302 -0.094307 -0.092461   \n",
       "1      0.594136 -0.099683  0.170995 -0.104191 -0.107329 -0.089034 -0.090868   \n",
       "2      0.816152 -0.095282  0.000230 -0.100147 -0.103879 -0.089214 -0.085673   \n",
       "3      0.733466 -0.088834  0.273690 -0.114222 -0.116357 -0.060485 -0.080096   \n",
       "4      0.483382 -0.094944  0.286158 -0.101178 -0.104258 -0.088571 -0.088850   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198  0.536659 -0.103923  0.127736 -0.109245 -0.112429 -0.094747 -0.092203   \n",
       "48199  0.396370 -0.094585  0.299330 -0.105385 -0.110923 -0.092127 -0.094027   \n",
       "48200  0.024202 -0.066634  0.977552 -0.074948 -0.076043 -0.065239 -0.067072   \n",
       "48201  0.803109 -0.086178  0.036741 -0.109458 -0.115373 -0.039178 -0.054202   \n",
       "48202  0.855621 -0.078897  0.037913 -0.109265 -0.112099 -0.038798 -0.056705   \n",
       "\n",
       "            126       127       128       129       130       131       132  \\\n",
       "0     -0.110055 -0.072320 -0.101630 -0.110362 -0.113396  0.400050 -0.090653   \n",
       "1     -0.105538 -0.057181 -0.096199 -0.107437 -0.111155  0.405383 -0.086804   \n",
       "2     -0.102612 -0.048358 -0.092067 -0.100856 -0.105592  0.657454 -0.085529   \n",
       "3     -0.114231 -0.073862 -0.098380 -0.112004 -0.122159  0.455277 -0.047073   \n",
       "4     -0.100128 -0.078800 -0.092968 -0.103294 -0.102136  0.284629 -0.084118   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48198 -0.108622 -0.075437 -0.098587 -0.096890 -0.110126  0.421573 -0.093679   \n",
       "48199 -0.107301 -0.077611 -0.098453 -0.099868 -0.112403  0.334659 -0.088464   \n",
       "48200 -0.078083 -0.072537 -0.073559 -0.016842 -0.084288  0.075517 -0.068157   \n",
       "48201 -0.111994 -0.058026 -0.090350 -0.102356 -0.124435  0.485146 -0.011179   \n",
       "48202 -0.111807 -0.042499 -0.086195 -0.116125 -0.126190  0.449037  0.012051   \n",
       "\n",
       "            133       134       135       136       137       138       139  \n",
       "0     -0.115911 -0.114397 -0.078690  0.011340 -0.056059  0.012302  0.078415  \n",
       "1     -0.110228 -0.110396 -0.084210 -0.006720 -0.076285  0.012299  0.078346  \n",
       "2     -0.106070 -0.105622 -0.088992  0.080794 -0.068035  0.034081  0.154974  \n",
       "3     -0.132610 -0.127592 -0.069297  0.386599 -0.114666  0.312522  0.101726  \n",
       "4     -0.102261 -0.112081 -0.079703  0.077863 -0.093023  0.055629  0.045722  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "48198 -0.110951 -0.117332 -0.088202  0.029138 -0.020140 -0.005769  0.105734  \n",
       "48199 -0.111661 -0.116378 -0.087172  0.112799 -0.044621  0.015924  0.092049  \n",
       "48200 -0.082042 -0.075769 -0.035932  0.073994 -0.071730 -0.055548  0.074910  \n",
       "48201 -0.137868 -0.138146 -0.082405  0.565141 -0.111353  0.437580  0.196572  \n",
       "48202 -0.144969 -0.139429 -0.067139  0.731363 -0.114612  0.458202  0.194715  \n",
       "\n",
       "[48203 rows x 140 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_sub = pd.DataFrame(pred.round(6))\n",
    "cite_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b4a096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite_sub.to_csv('../../../../../summary/output/submit/cite_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "738d9ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m CiteModel(feature_dims)\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcite_mlp_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(torch.load(f'/dss/dsshome1/02/di93zoj/valentina/open-problems-multimodal-3rd-solution/code/4.model/train/cite/cite_mlp_corr_svd_128_flg_donor_val_28', \u001b[39;00m\n\u001b[1;32m     25\u001b[0m                                 \u001b[38;5;66;03m#map_location='cuda:0'))  # cuda:0\u001b[39;00m\n\u001b[1;32m     27\u001b[0m result \u001b[38;5;241m=\u001b[39m test_loop(model, test_dataloader)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:811\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    810\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1174\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1172\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1173\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1174\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1144\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1143\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1144\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:1118\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1114\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1118\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1119\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1120\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1123\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:218\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 218\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:183\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 183\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:174\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    172\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on CUDA device \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    175\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but torch.cuda.device_count() is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    176\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load with map_location to map your storages \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    177\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto an existing device.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device."
     ]
    }
   ],
   "source": [
    "# model #16: cite_mlp_corr_svd_128_flg_donor_val_30\n",
    "pred_16 = np.zeros([48203, 140])\n",
    "\n",
    "i = 'cite_mlp_corr_svd_128_flg_donor_val_30'\n",
    "        \n",
    "test_file = model_feat_dict[i][0]\n",
    "test_weight = model_feat_dict[i][1]\n",
    "X_test = pd.read_pickle(cite_feature_path  + test_file)\n",
    "# columns = X_test.columns   # for SHAP\n",
    "X_test = np.array(X_test)\n",
    "feature_dims = X_test.shape[1]\n",
    "\n",
    "test_ds = CiteDataset_test(X_test)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=128, pin_memory=True, \n",
    "                              shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "if 'mish' in i:\n",
    "    model = CiteModel_mish(feature_dims)\n",
    "else:\n",
    "    model = CiteModel(feature_dims)\n",
    "    \n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'{cite_mlp_path}/{i}'))\n",
    "#model.load_state_dict(torch.load(f'/dss/dsshome1/02/di93zoj/valentina/open-problems-multimodal-3rd-solution/code/4.model/train/cite/cite_mlp_corr_svd_128_flg_donor_val_28', \n",
    "                                #map_location='cuda:0'))  # cuda:0\n",
    "\n",
    "result = test_loop(model, test_dataloader).astype(np.float32)\n",
    "# result = std(result) * test_weight / weight_sum  # double check \n",
    "pred_16 += result\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "        \n",
    "pd.DataFrame(pred_16)   # double check train_cite_targets.h5  -> omnipath -> then maybe shap\n",
    "# TODO SHAP on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d891d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(cite_feature_path  + test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25706cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# X_train for model #16: 'X_svd_128.pickle'\n",
    "X_train = pd.read_pickle(cite_feature_path  + 'X_svd_128.pickle')\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# X_train = X_train.to('cuda')\n",
    "\n",
    "# X_test for model #16:\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# X_test = X_test.to('cuda')\n",
    "\n",
    "# Explainer, KernelExplainer, don't rely on differentiable model\n",
    "# shap beeswarm -> screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SHAP values\n",
    "# explainer = shap.DeepExplainer(model, X_train)     # Warning: unrecognized nn.Module: LayerNorm\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "# shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a67075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/70510341/shap-values-with-pytorch-kernelexplainer-vs-deepexplainer\n",
    "# Get features\n",
    "train_features_df = ... # pandas dataframe\n",
    "test_features_df = ... # pandas dataframe\n",
    "\n",
    "# Define function to wrap model to transform data to tensor\n",
    "f = lambda x: model( torch.from_numpy(x) ).detach().numpy()   # model_list[0]\n",
    "\n",
    "# Convert my pandas dataframe to numpy\n",
    "# data = test_features_df.to_numpy(dtype=np.float32)\n",
    "data = X_train\n",
    "\n",
    "# The explainer doesn't like tensors, hence the f function\n",
    "explainer = shap.KernelExplainer(f, data)\n",
    "\n",
    "# Get the shap values from my test data\n",
    "shap_values = explainer.shap_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03186d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: no tensor\n",
    "# X_train: np.array\n",
    "# X_test: np.array\n",
    "\n",
    "explainer = shap.KernelExplainer(model, X_train)    #, keep_index=True)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9be56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tensor = torch.from_numpy(X_train).to('cuda')\n",
    "# explainer = shap.Explainer(model, X_tensor)\n",
    "# shap_values = explainer.explain_row(X_tensor, max_evals, main_effects, error_bounds, outputs, silent )  # X_test?\n",
    "# shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840c942",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_cite_targets = pd.read_hdf('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/neurips_competition_data/train_cite_targets.h5')    # train_cite_targets.h5 - Surface protein levels for the same cells that have been dsb normalized.\n",
    "print(train_cite_targets.shape)   # more rows; how to match rows -> metadata for cell_id\n",
    "train_cite_targets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0f8f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/neurips_competition_data/metadata.csv')\n",
    "metadata[metadata['cell_id'].isin(['45006fe3e4c8','d02759a80ba2','c016c6b0efa5','ba7f733a4f75','fbcf2443ffb2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2376a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# filter dataset by IDs in list\n",
    "metadata_filtered = metadata[metadata['cell_id'].isin(train_cite_targets.index.values)].set_index('cell_id')\n",
    "metadata_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c47ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_cite_targets = ad.AnnData(train_cite_targets, obs=metadata_filtered)\n",
    "train_cite_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ef8bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# run after preprocessing below\n",
    "# cite_sub['cell_id']\n",
    "pd.DataFrame(cite_sub)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dfe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cell_ids = train_cite_targets.obs.index.tolist()\n",
    "target_cell_ids[:10]  # 70988\n",
    "\n",
    "print(set(pd.DataFrame(cite_sub)[0]).issubset(set(target_cell_ids)))\n",
    "\n",
    "\n",
    "mask = pd.DataFrame(cite_sub)[0].isin(target_cell_ids)\n",
    "result = pd.DataFrame(cite_sub).loc[mask]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(test_sub_ids).issubset(set(target_cell_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357169d",
   "metadata": {},
   "source": [
    "## Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_name = [\n",
    "    'multi_mlp_all_con_16',\n",
    "    'multi_mlp_all_con_32', \n",
    "    'multi_mlp_all_binary_16',\n",
    "    'multi_mlp_all_last_cluster',\n",
    "    'multi_mlp_all_lsi_w2v_col_128_flg',\n",
    "    'multi_mlp_all_lsi_w2v_128_flg',\n",
    "    'multi_mlp_all_lsi_128_flg',\n",
    "    'multi_mlp_all_lsi_w2v_col_64_flg',\n",
    "    'multi_mlp_all_lsi_w2v_64_flg',\n",
    "    'multi_mlp_all_lsi_64_flg',\n",
    "    'multi_mlp_all_okapi_128_flg',\n",
    "    'multi_mlp_all_okapi_64_flg',\n",
    "    'multi_mlp_all_colmean_64_flg',\n",
    "    'multi_mlp_corr_con_16_flg',\n",
    "    'multi_mlp_corr_con_32_flg',\n",
    "    'multi_mlp_corr_binary_16',\n",
    "    'multi_mlp_corr_lsi_add_lc_svd_flg',\n",
    "    \n",
    "    'multi_mlp_corr_lsi_w2v_col_128_flg',\n",
    "    'multi_mlp_corr_lsi_w2v_col_64_flg',\n",
    "    'multi_mlp_corr_lsi_w2v_128_flg',\n",
    "    'multi_mlp_corr_lsi_w2v_64_flg',\n",
    "    \n",
    "    'multi_mlp_corr_lsi_128_flg',\n",
    "    'multi_mlp_corr_lsi_64_flg',\n",
    "    \n",
    "    'multi_mlp_corr_colmean_64_flg',\n",
    "    'multi_mlp_corr_okapi_w2v_64_flg',\n",
    "    'multi_mlp_corr_okapi_64_flg',\n",
    "    \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = []\n",
    "\n",
    "for i in mlp_model_name:\n",
    "    for num, j in enumerate(os.listdir(multi_mlp_path)):\n",
    "        if i in j:\n",
    "            model_name_list.append(j)\n",
    "\n",
    "print(len(model_name_list))\n",
    "model_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14268bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [2.5, 2.5, 2.5, 1.2, 1.2, 1.2, 1, \n",
    "          1.5, 1.5, 2.5, 0.5, 0.5, 0.5, \n",
    "          2.5, 2.5, 1.8, 0.8, 1, 0.8, 1 ,0.8, 1, 0.3, \n",
    "          0.3, 0.3, 0.3, 0.2, 0.2, 0.2]\n",
    "weight_sum = np.array(weight).sum()\n",
    "weight_sum\n",
    "\n",
    "model_feat_dict = {model_name_list[0]:['multi_test_con_16.pickle', 2.5],\n",
    "                   model_name_list[1]:['multi_test_con_32.pickle', 2.5],\n",
    "                   model_name_list[2]:['multi_test_binary_16.pickle', 2.5],\n",
    "                   \n",
    "                   model_name_list[3]:['multi_test_okapi_64_last_cluster.pickle', 1.2],\n",
    "                   model_name_list[4]:['multi_test_lsi_w2v_col_128.pickle', 1.2],\n",
    "                   model_name_list[5]:['multi_test_lsi_w2v_128.pickle', 1.2],\n",
    "                   model_name_list[6]:['multi_test_okapi_lsi_128.pickle', 1],\n",
    "                   \n",
    "                   model_name_list[7]:['multi_test_lsi_w2v_col_64.pickle', 1.5],\n",
    "                   model_name_list[8]:['multi_test_lsi_w2v_64.pickle', 1.5],\n",
    "                   model_name_list[9]:['multi_test_okapi_lsi_64.pickle', 2.5],\n",
    "                   \n",
    "                   model_name_list[10]:['multi_test_okapi_feature_128.pickle', 0.5],\n",
    "                   model_name_list[11]:['multi_test_okapi_feature_64.pickle', 0.5],\n",
    "                   model_name_list[12]:['multi_test_okapi_w2v_col_64.pickle', 0.5],\n",
    "                   \n",
    "                   model_name_list[13]:['multi_test_con_16.pickle', 2.5],\n",
    "                   model_name_list[14]:['multi_test_con_32.pickle', 2.5],\n",
    "                   model_name_list[15]:['multi_test_binary_16.pickle', 1.8],\n",
    "                   model_name_list[16]:['multi_test_lc_addsvd_64.pickle', 0.8],\n",
    "                   \n",
    "                   model_name_list[17]:['multi_test_lsi_w2v_col_128.pickle', 1],\n",
    "                   model_name_list[18]:['multi_test_lsi_w2v_col_64.pickle', 0.8],\n",
    "                   model_name_list[19]:['multi_test_lsi_w2v_128.pickle', 1],\n",
    "                   model_name_list[20]:['multi_test_lsi_w2v_64.pickle', 0.8],\n",
    "                   model_name_list[21]:['multi_test_okapi_lsi_128.pickle', 1],\n",
    "                   model_name_list[22]:['multi_test_okapi_lsi_64.pickle', 0.3],\n",
    "                   \n",
    "                   model_name_list[23]:['multi_test_okapi_w2v_col_64.pickle', 0.3],\n",
    "                   model_name_list[24]:['multi_test_okapi_w2v_64.pickle', 0.3],\n",
    "                   model_name_list[25]:['multi_test_okapi_feature_64.pickle', 0.3],\n",
    "                   \n",
    "                   'lsi_128':['multi_test_okapi_lsi_128.pickle', 0.2],\n",
    "                   'lsi_64':['multi_test_okapi_lsi_64.pickle', 0.2],\n",
    "                   'lsi_w2v_col_64':['multi_test_lsi_w2v_col_64.pickle', 0.2],\n",
    "                  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5dd547",
   "metadata": {},
   "source": [
    "### multi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, feature, target):\n",
    "        \n",
    "        self.feature = feature\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        d = {\n",
    "            \"X\": self.feature[index],\n",
    "            \"y\" : self.target[index],\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a541b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDataset_test(Dataset):\n",
    "    \n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        d = {\n",
    "            \"X\": self.feature[index]\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation_score_torch_faster(y_true, y_pred):\n",
    "    \"\"\"Compute the correlation between each rows of the y_true and y_pred tensors.\n",
    "    Compatible with backpropagation.\n",
    "    \"\"\"\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1)[:,None]\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:,None]\n",
    "    cov_tp = torch.sum(y_true_centered*y_pred_centered, dim=1)/(y_true.shape[1]-1)\n",
    "    var_t = torch.sum(y_true_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    var_p = torch.sum(y_pred_centered**2, dim=1)/(y_true.shape[1]-1)\n",
    "    return cov_tp/torch.sqrt(var_t*var_p)\n",
    "\n",
    "def correl_loss(pred, tgt):\n",
    "    \"\"\"Loss for directly optimizing the correlation.\n",
    "    \"\"\"\n",
    "    return -torch.mean(partial_correlation_score_torch_faster(tgt, pred))\n",
    "\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Scores the predictions according to the competition rules. \n",
    "    \n",
    "    It is assumed that the predictions are not constant.\n",
    "    \n",
    "    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    if y_true.shape != y_pred.shape: raise ValueError(\"Shapes are different.\")\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_num):\n",
    "        super(MultiModel, self).__init__()\n",
    "        \n",
    "        self.layer_seq_128 = nn.Sequential(nn.Linear(feature_num, 128),\n",
    "                                           nn.LayerNorm(128),\n",
    "                                           nn.ReLU(),\n",
    "                                      )\n",
    "        \n",
    "        self.layer_seq_64 = nn.Sequential(nn.Linear(128, 64),\n",
    "                                           nn.LayerNorm(64),\n",
    "                                           nn.ReLU(),\n",
    "                                      )\n",
    "        \n",
    "        self.layer_seq_32 = nn.Sequential(nn.Linear(64, 32),\n",
    "                                   nn.LayerNorm(32),\n",
    "                                   nn.ReLU(),\n",
    "                              )\n",
    "        \n",
    "        self.layer_seq_8 = nn.Sequential(nn.Linear(32, 8),\n",
    "                                         nn.LayerNorm(8),\n",
    "                                         nn.ReLU(),\n",
    "                                      )\n",
    "        \n",
    "        self.head = nn.Linear(128 + 64 + 32 + 8, target_num)\n",
    "                   \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        X_128 = self.layer_seq_128(X)\n",
    "        X_64 = self.layer_seq_64(X_128)\n",
    "        X_32 = self.layer_seq_32(X_64)\n",
    "        X_8 = self.layer_seq_8(X_32)\n",
    "        X = torch.cat([X_128, X_64, X_32, X_8], axis = 1)\n",
    "        out = self.head(X)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, loader, epoch):\n",
    "    \n",
    "    losses, lrs = [], []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with tqdm(total=len(loader),unit=\"batch\") as pbar:\n",
    "        pbar.set_description(f\"Epoch{epoch}\")\n",
    "        \n",
    "        for d in loader:\n",
    "            X = d['X'].to(device).float()\n",
    "            y = d['y'].to(device)\n",
    "            \n",
    "            logits = model(X)\n",
    "            #loss = correl_loss(logits, y)\n",
    "            loss = torch.sqrt(loss_fn(logits, y))\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix({\"loss\":loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bffb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop(model, loader, y_val):\n",
    "    \n",
    "    model.eval()\n",
    "    partial_correlation_scores = []\n",
    "    oof_pred = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            val_X = d['X'].to(device).float()\n",
    "            val_y = d['y'].to(device)\n",
    "            logits = model(val_X)\n",
    "            #oof_pred.append(logits.detach().cpu().numpy())\n",
    "            oof_pred.append(logits)\n",
    "    \n",
    "    y_val = torch.tensor(y_val).to(device)\n",
    "    logits = torch.cat(oof_pred)\n",
    "    #print(logits.shape, y_val.shape)\n",
    "    loss = torch.sqrt(loss_fn(logits, y_val))\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    predicts=[]\n",
    "\n",
    "    for d in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            X = d['X'].to(device).float()\n",
    "            logits = model(X)\n",
    "            predicts.append(logits.detach().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef424a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros([55935, 23418])\n",
    "svd = pickle.load(open(multi_target_path + 'multi_all_target_128.pkl', 'rb'))\n",
    "\n",
    "for num, i in enumerate(model_feat_dict.keys()):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    if 'mlp' in i:\n",
    "        \n",
    "        test_file = model_feat_dict[i][0]\n",
    "        test_weight = model_feat_dict[i][1]\n",
    "        X_test = pd.read_pickle(multi_feature_path  + test_file)    \n",
    "        X_test = np.array(X_test)\n",
    "        feature_dims = X_test.shape[1]\n",
    "\n",
    "        test_ds = MultiDataset_test(X_test)\n",
    "        test_dataloader = DataLoader(test_ds, batch_size=128, pin_memory=True, \n",
    "                                     shuffle=False, drop_last=False, num_workers=4)\n",
    "        \n",
    "        if 'all' in i:\n",
    "            target_num = 23418\n",
    "        else:\n",
    "            target_num = 128\n",
    "        \n",
    "        model = MultiModel(feature_dims)    \n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(f'{multi_mlp_path}/{i}'))\n",
    "        \n",
    "        result = test_loop(model, test_dataloader).astype(np.float32)\n",
    "        \n",
    "        if 'all' not in i:\n",
    "            result = result@svd.components_\n",
    "                \n",
    "        result = result * test_weight / weight_sum\n",
    "        pred += result\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    else:\n",
    "        test_file = model_feat_dict[i][0]\n",
    "        test_weight = model_feat_dict[i][1]\n",
    "        X_test = pd.read_pickle(multi_feature_path  + test_file)\n",
    "        \n",
    "        cb_pred = np.zeros([55935, 128])\n",
    "        \n",
    "        for t in tqdm(range(128)): \n",
    "            cb_model_path = [j for j in os.listdir(multi_cb_path) if f'cb_{t}_{i}' in j][0]\n",
    "            cb = pickle.load(open(multi_cb_path + cb_model_path, 'rb'))\n",
    "            cb_pred[:,t] = cb.predict(X_test)\n",
    "            \n",
    "        cb_pred = cb_pred.astype(np.float32)\n",
    "        cb_pred = cb_pred@svd.components_\n",
    "        pred += cb_pred * test_weight / weight_sum\n",
    "        \n",
    "        #del cb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sub = pd.DataFrame(pred.round(6)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9babf5",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba370ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_path = lrz_path + 'input/preprocess/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdddee",
   "metadata": {},
   "source": [
    "#### first: fix cite output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18b30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "test_sub_ids = np.load(preprocess_path + \"test_cite_inputs_idxcol.npz\", allow_pickle=True)\n",
    "test_sub_ids = test_sub_ids[\"index\"]\n",
    "test_raw_ids = np.load(preprocess_path + \"test_cite_raw_inputs_idxcol.npz\", allow_pickle=True)\n",
    "test_raw_ids = test_raw_ids[\"index\"]\n",
    "test_raw_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdbc90",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(test_raw_ids)\n",
    "cite_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cite_df = pd.DataFrame(test_sub_ids, columns = ['cell_id'])\n",
    "cite_sub['cell_id'] = test_raw_ids# .tolist()\n",
    "test_cite_df = test_cite_df.merge(cite_sub, on = 'cell_id', how = 'left')\n",
    "test_cite_df.fillna(0, inplace = True)\n",
    "# test_cite_df.drop(['cell_id'], axis = 1, inplace = True)\n",
    "\n",
    "cite_sub = test_cite_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34515d",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a570f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/neurips_competition_data/sample_submission.csv')  \n",
    "eval_ids = pd.read_csv('/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93zoj/neurips_competition_data/evaluation_ids.csv') \n",
    "\n",
    "cite_cols = pd.read_csv(preprocess_path + \"cite_test_cols.csv\") \n",
    "cite_index = pd.read_csv(preprocess_path + \"cite_test_indexs.csv\") \n",
    "cite_index.columns = ['cell_id']\n",
    "\n",
    "# multi_cols = pd.read_csv(preprocess_path + \"multi/multi_test_cols.csv\") \n",
    "# multi_index = pd.read_csv(preprocess_path + \"multi/multi_test_indexs.csv\") \n",
    "# multi_index.columns = ['cell_id']\n",
    "\n",
    "submission = pd.Series(name='target',index=pd.MultiIndex.from_frame(eval_ids), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244bf18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163d973",
   "metadata": {},
   "source": [
    "### multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a89058",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sub = np.array(multi_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_dict = dict((k,v) for v,k in enumerate(np.array(multi_index['cell_id'])))\n",
    "assert len(cell_dict)  == len(multi_index['cell_id'])\n",
    "\n",
    "gene_dict = dict((k,v) for v,k in enumerate(np.array(multi_cols['gene_id']))) \n",
    "assert len(gene_dict)  == len(multi_cols['gene_id'])\n",
    "\n",
    "eval_ids_cell_num = eval_ids.cell_id.apply(lambda x:cell_dict.get(x, -1))\n",
    "eval_ids_gene_num = eval_ids.gene_id.apply(lambda x:gene_dict.get(x, -1))\n",
    "\n",
    "valid_multi_rows = (eval_ids_gene_num !=-1) & (eval_ids_cell_num!=-1)\n",
    "submission.iloc[valid_multi_rows] = multi_sub[eval_ids_cell_num[valid_multi_rows].to_numpy(),\n",
    "                                                 eval_ids_gene_num[valid_multi_rows].to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb45b68",
   "metadata": {},
   "source": [
    "### cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_sub = np.array(cite_sub)\n",
    "cite_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_dict = dict((k,v) for v,k in enumerate(np.array(cite_index['cell_id'])))\n",
    "assert len(cell_dict)  == len(cite_index['cell_id'])\n",
    "\n",
    "gene_dict = dict((k,v) for v,k in enumerate(np.array(cite_cols['gene_id']))) \n",
    "assert len(gene_dict)  == len(cite_cols['gene_id'])\n",
    "\n",
    "eval_ids_cell_num = eval_ids.cell_id.apply(lambda x:cell_dict.get(x, -1))\n",
    "eval_ids_gene_num = eval_ids.gene_id.apply(lambda x:gene_dict.get(x, -1))\n",
    "\n",
    "valid_multi_rows = (eval_ids_gene_num !=-1) & (eval_ids_cell_num!=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133db064",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[valid_multi_rows] = cite_sub[eval_ids_cell_num[valid_multi_rows].to_numpy(),\n",
    "                                                 eval_ids_gene_num[valid_multi_rows].to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a858b",
   "metadata": {},
   "source": [
    "### make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c70592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = submission.round(6)\n",
    "submission = pd.DataFrame(submission, columns = ['target'])\n",
    "submission = submission.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb704981",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['row_id', 'target']]#.dropna()        #.to_csv(output_path + 'submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c open-problems-multimodal -f $sub_name_csv -m $message"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
